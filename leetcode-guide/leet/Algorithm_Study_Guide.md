# The Ultimate Algorithm Study Guide ðŸš€

## Table of Contents

### I. Two Pointer Patterns / Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)
- [11. Container With Most Water](#container-with-most-water)
- [15. 3Sum](#3sum)
- [16. 3Sum Closest](#3sum-closest)
- [18. 4Sum](#4sum)
- [167. Two Sum II - Input Array Is Sorted](#two-sum-ii---input-array-is-sorted)
- [349. Intersection of Two Arrays](#intersection-of-two-arrays)
- [881. Boats to Save People](#boats-to-save-people)
- [977. Squares of a Sorted Array](#squares-of-a-sorted-array)
- [259. 3Sum Smaller](#3sum-smaller)

### I. Two Pointer Patterns / Pattern 2: Two Pointers - Fast & Slow (Cycle Detection)
- [141. Linked List Cycle](#linked-list-cycle)
- [202. Happy Number](#happy-number)
- [287. Find the Duplicate Number](#find-the-duplicate-number)
- [392. Is Subsequence](#is-subsequence)

### I. Two Pointer Patterns / Pattern 3: Two Pointers - Fixed Separation (Nth Node from End)
- [19. Remove Nth Node From End of List](#remove-nth-node-from-end-of-list)
- [876. Middle of the Linked List](#middle-of-the-linked-list)
- [2095. Delete the Middle Node of a Linked List](#delete-the-middle-node-of-a-linked-list)

### I. Two Pointer Patterns / Pattern 4: Two Pointers - In-place Array Modification
- [26. Remove Duplicates from Sorted Array](#remove-duplicates-from-sorted-array)
- [27. Remove Element](#remove-element)
- [75. Sort Colors](#sort-colors)
- [80. Remove Duplicates from Sorted Array II](#remove-duplicates-from-sorted-array-ii)
- [283. Move Zeroes](#move-zeroes)
- [443. String Compression](#string-compression)
- [905. Sort Array By Parity](#sort-array-by-parity)
- [2337. Move Pieces to Obtain a String](#move-pieces-to-obtain-a-string)
- [2938. Separate Black and White Balls](#separate-black-and-white-balls)

### I. Two Pointer Patterns / Pattern 5: Two Pointers - String Comparison
- [844. Backspace String Compare](#backspace-string-compare)
- [1598. Crawler Log Folder](#crawler-log-folder)
- [2390. Removing Stars From a String](#removing-stars-from-a-string)

### I. Two Pointer Patterns / Pattern 6: Two Pointers - Expanding From Center (Palindromes)
- [5. Longest Palindromic Substring](#longest-palindromic-substring)
- [647. Palindromic Substrings](#palindromic-substrings)
- [9. Palindrome Number](#palindrome-number)
- [131. Palindrome Partitioning](#palindrome-partitioning)
- [132. Palindrome Partitioning II](#palindrome-partitioning-ii)

### I. Two Pointer Patterns / Pattern 7: Two Pointers - String Reversal
- [151. Reverse Words in a String](#reverse-words-in-a-string)
- [344. Reverse String](#reverse-string)
- [345. Reverse Vowels of a String](#reverse-vowels-of-a-string)
- [541. Reverse String II](#reverse-string-ii)

### II. Sliding Window Patterns / Pattern 10: Sliding Window - Monotonic Queue for Max/Min
- [239. Sliding Window Maximum](#sliding-window-maximum)
- [862. Shortest Subarray with Sum at Least K](#shortest-subarray-with-sum-at-least-k)
- [1696. Jump Game VI](#jump-game-vi)

### II. Sliding Window Patterns / Pattern 11: Sliding Window - Character Frequency Matching
- [1.TwoSum](#twosum)
- [438. Find All Anagrams in a String](#find-all-anagrams-in-a-string)
- [567. Permutation in String](#permutation-in-string)

### II. Sliding Window Patterns / Pattern 8: Sliding Window - Fixed Size (Subarray Calculation)
- [346. Moving Average from Data Stream](#moving-average-from-data-stream)
- [643. Maximum Average Subarray I](#maximum-average-subarray-i)
- [2985. Calculate Compressed Mean](#calculate-compressed-mean)
- [3254. Find the Power of K-Size Subarrays I](#find-the-power-of-k-size-subarrays-i)
- [3318. Find X-Sum of All K-Long Subarrays I](#find-x-sum-of-all-k-long-subarrays-i)

### II. Sliding Window Patterns / Pattern 9: Sliding Window - Variable Size (Condition-Based)
- [3. Longest Substring Without Repeating Characters](#longest-substring-without-repeating-characters)
- [76. Minimum Window Substring](#minimum-window-substring)
- [209. Minimum Size Subarray Sum](#minimum-size-subarray-sum)
- [219. Contains Duplicate II](#contains-duplicate-ii)
- [424. Longest Repeating Character Replacement](#longest-repeating-character-replacement)
- [713. Subarray Product Less Than K](#subarray-product-less-than-k)
- [904. Fruit Into Baskets](#fruit-into-baskets)
- [1004. Max Consecutive Ones III](#max-consecutive-ones-iii)
- [1438. Longest Continuous Subarray With Absolute Diff Less Than or Equal to Limit](#longest-continuous-subarray-with-absolute-diff-less-than-or-equal-to-limit)
- [1493. Longest Subarray of 1's After Deleting One Element](#longest-subarray-of-1s-after-deleting-one-element)
- [1658. Minimum Operations to Reduce X to Zero](#minimum-operations-to-reduce-x-to-zero)
- [1838. Frequency of the Most Frequent Element](#frequency-of-the-most-frequent-element)
- [2461. Maximum Sum of Distinct Subarrays With Length K](#maximum-sum-of-distinct-subarrays-with-length-k)
- [2516. Take K of Each Character From Left and Right](#take-k-of-each-character-from-left-and-right)
- [2762. Continuous Subarrays](#continuous-subarrays)
- [2779. Maximum Beauty of an Array After Applying Operation](#maximum-beauty-of-an-array-after-applying-operation)
- [2981. Find Longest Special Substring That Occurs Thrice I](#find-longest-special-substring-that-occurs-thrice-i)
- [3026. Maximum Good Subarray Sum](#maximum-good-subarray-sum)
- [3346. Maximum Frequency of an Element After Performing Operations I](#maximum-frequency-of-an-element-after-performing-operations-i)
- [3347. Maximum Frequency of an Element After Performing Operations II](#maximum-frequency-of-an-element-after-performing-operations-ii)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 12: Tree BFS - Level Order Traversal
- [102. Binary Tree Level Order Traversal](#binary-tree-level-order-traversal)
- [103. Binary Tree Zigzag Level Order Traversal](#binary-tree-zigzag-level-order-traversal)
- [199. Binary Tree Right Side View](#binary-tree-right-side-view)
- [515. Find Largest Value in Each Tree Row](#find-largest-value-in-each-tree-row)
- [1161. Maximum Level Sum of a Binary Tree](#maximum-level-sum-of-a-binary-tree)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 13: Tree DFS - Recursive Preorder Traversal
- [100. Same Tree](#same-tree)
- [101. Symmetric Tree](#symmetric-tree)
- [105. Construct Binary Tree from Preorder and Inorder Traversal](#construct-binary-tree-from-preorder-and-inorder-traversal)
- [114. Flatten Binary Tree to Linked List](#flatten-binary-tree-to-linked-list)
- [226. Invert Binary Tree](#invert-binary-tree)
- [257. Binary Tree Paths](#binary-tree-paths)
- [988. Smallest String Starting From Leaf](#smallest-string-starting-from-leaf)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 14: Tree DFS - Recursive Inorder Traversal
- [94. Binary Tree Inorder Traversal](#binary-tree-inorder-traversal)
- [98. Validate Binary Search Tree](#validate-binary-search-tree)
- [173. Binary Search Tree Iterator](#binary-search-tree-iterator)
- [230. Kth Smallest Element in a BST](#kth-smallest-element-in-a-bst)
- [501. Find Mode in Binary Search Tree](#find-mode-in-binary-search-tree)
- [530. Minimum Absolute Difference in BST](#minimum-absolute-difference-in-bst)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 15: Tree DFS - Recursive Postorder Traversal
- [104. Maximum Depth of Binary Tree](#maximum-depth-of-binary-tree)
- [110. Balanced Binary Tree](#balanced-binary-tree)
- [124. Binary Tree Maximum Path Sum](#binary-tree-maximum-path-sum)
- [145. Binary Tree Postorder Traversal](#binary-tree-postorder-traversal)
- [337. House Robber III](#house-robber-iii)
- [366. Find Leaves of Binary Tree](#find-leaves-of-binary-tree)
- [543. Diameter of Binary Tree](#diameter-of-binary-tree)
- [863. All Nodes Distance K in Binary Tree](#all-nodes-distance-k-in-binary-tree)
- [1110. Delete Nodes And Return Forest](#delete-nodes-and-return-forest)
- [2458. Height of Binary Tree After Subtree Removal Queries](#height-of-binary-tree-after-subtree-removal-queries)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 16: Tree - Lowest Common Ancestor (LCA) Finding
- [235. Lowest Common Ancestor of a Binary Search Tree](#lowest-common-ancestor-of-a-binary-search-tree)
- [236. Lowest Common Ancestor of a Binary Tree](#lowest-common-ancestor-of-a-binary-tree)

### III. Tree Traversal Patterns (DFS & BFS) / Pattern 17: Tree - Serialization and Deserialization
- [297. Serialize and Deserialize Binary Tree](#serialize-and-deserialize-binary-tree)
- [572. Subtree of Another Tree](#subtree-of-another-tree)
- [652. Find Duplicate Subtrees](#find-duplicate-subtrees)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 18: Graph DFS - Connected Components / Island Counting
- [130. Surrounded Regions](#surrounded-regions)
- [200. Number of Islands](#number-of-islands)
- [417. Pacific Atlantic Water Flow](#pacific-atlantic-water-flow)
- [547. Number of Provinces](#number-of-provinces)
- [695. Max Area of Island](#max-area-of-island)
- [733. Flood Fill](#flood-fill)
- [841. Keys and Rooms](#keys-and-rooms)
- [1020. Number of Enclaves](#number-of-enclaves)
- [1254. Number of Closed Islands](#number-of-closed-islands)
- [1905. Count Sub Islands](#count-sub-islands)
- [2101. Detonate the Maximum Bombs](#detonate-the-maximum-bombs)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 19: Graph BFS - Connected Components / Island Counting
- [127. Word Ladder](#word-ladder)
- [542. 01 Matrix](#01-matrix)
- [994. Rotting Oranges](#rotting-oranges)
- [1091. Shortest Path in Binary Matrix](#shortest-path-in-binary-matrix)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 20: Graph DFS - Cycle Detection (Directed Graph)
- [207. Course Schedule](#course-schedule)
- [210. Course Schedule II](#course-schedule-ii)
- [802. Find Eventual Safe States](#find-eventual-safe-states)
- [1059. All Paths from Source Lead to Destination](#all-paths-from-source-lead-to-destination)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 21: Graph BFS - Topological Sort (Kahn's Algorithm)
- [207. Course Schedule](#course-schedule)
- [210. Course Schedule II](#course-schedule-ii)
- [269. Alien Dictionary](#alien-dictionary)
- [310. Minimum Height Trees](#minimum-height-trees)
- [444. Sequence Reconstruction](#sequence-reconstruction)
- [1136. Parallel Courses](#parallel-courses)
- [1857. Largest Color Value in a Directed Graph](#largest-color-value-in-a-directed-graph)
- [2050. Parallel Courses III](#parallel-courses-iii)
- [2115. Find All Possible Recipes from Given Supplies](#find-all-possible-recipes-from-given-supplies)
- [2392. Build a Matrix With Conditions](#build-a-matrix-with-conditions)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 22: Graph - Deep Copy / Cloning
- [133. Clone Graph](#clone-graph)
- [1334. Find the City With the Smallest Number of Neighbors at a Threshold Distance](#find-the-city-with-the-smallest-number-of-neighbors-at-a-threshold-distance)
- [138. Copy List with Random Pointer](#copy-list-with-random-pointer)
- [1490. Clone N-ary Tree](#clone-n-ary-tree)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 23: Graph - Shortest Path (Dijkstra's Algorithm)
- [743. Network Delay Time](#network-delay-time)
- [778. Swim in Rising Water](#swim-in-rising-water)
- [1514. Path with Maximum Probability](#path-with-maximum-probability)
- [1631. Path With Minimum Effort](#path-with-minimum-effort)
- [1976. Number of Ways to Arrive at Destination](#number-of-ways-to-arrive-at-destination)
- [2045. Second Minimum Time to Reach Destination](#second-minimum-time-to-reach-destination)
- [2203. Minimum Weighted Subgraph With the Required Paths](#minimum-weighted-subgraph-with-the-required-paths)
- [2290. Minimum Obstacle Removal to Reach Corner](#minimum-obstacle-removal-to-reach-corner)
- [2577. Minimum Time to Visit a Cell In a Grid](#minimum-time-to-visit-a-cell-in-a-grid)
- [2812. Find the Safest Path in a Grid](#find-the-safest-path-in-a-grid)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 24: Graph - Shortest Path (Bellman-Ford / BFS+K)
- [787. Cheapest Flights Within K Stops](#cheapest-flights-within-k-stops)
- [743. Network Delay Time](#network-delay-time)

### IV. Graph Traversal Patterns (DFS & BFS) / Pattern 25: Graph - Union-Find (Disjoint Set Union - DSU)
- [200. Number of Islands](#number-of-islands)
- [261. Graph Valid Tree](#graph-valid-tree)
- [305. Number of Islands II](#number-of-islands-ii)
- [323. Number of Connected Components in an Undirected Graph](#number-of-connected-components-in-an-undirected-graph)
- [547. Number of Provinces](#number-of-provinces)
- [684. Redundant Connection](#redundant-connection)
- [721. Accounts Merge](#accounts-merge)
- [737. Sentence Similarity II](#sentence-similarity-ii)
- [947. Most Stones Removed with Same Row or Column](#most-stones-removed-with-same-row-or-column)
- [952. Largest Component Size by Common Factor](#largest-component-size-by-common-factor)
- [959. Regions Cut By Slashes](#regions-cut-by-slashes)
- [1101. The Earliest Moment When Everyone Become Friends](#the-earliest-moment-when-everyone-become-friends)

### IX. Binary Search Patterns / Pattern 53: Binary Search - On Sorted Array/List
- [35. Search Insert Position](#search-insert-position)
- [69. Sqrt(x)](#sqrtx)
- [74. Search a 2D Matrix](#search-a-2d-matrix)
- [278. First Bad Version](#first-bad-version)
- [374. Guess Number Higher or Lower](#guess-number-higher-or-lower)
- [540. Single Element in a Sorted Array](#single-element-in-a-sorted-array)
- [704. Binary Search](#binary-search)
- [1539. Kth Missing Positive Number](#kth-missing-positive-number)

### IX. Binary Search Patterns / Pattern 54: Binary Search - Find Min/Max in Rotated Sorted Array
- [33. Search in Rotated Sorted Array](#search-in-rotated-sorted-array)
- [81. Search in Rotated Sorted Array II](#search-in-rotated-sorted-array-ii)
- [153. Find Minimum in Rotated Sorted Array](#find-minimum-in-rotated-sorted-array)
- [162. Find Peak Element](#find-peak-element)
- [852. Peak Index in a Mountain Array](#peak-index-in-a-mountain-array)
- [1095. Find in Mountain Array](#find-in-mountain-array)

### IX. Binary Search Patterns / Pattern 55: Binary Search - On Answer / Condition Function
- [410. Split Array Largest Sum](#split-array-largest-sum)
- [774. Minimize Max Distance to Gas Station](#minimize-max-distance-to-gas-station)
- [875. Koko Eating Bananas](#koko-eating-bananas)
- [1011. Capacity To Ship Packages Within D Days](#capacity-to-ship-packages-within-d-days)
- [1482. Minimum Number of Days to Make m Bouquets](#minimum-number-of-days-to-make-m-bouquets)
- [1760. Minimum Limit of Balls in a Bag](#minimum-limit-of-balls-in-a-bag)
- [2064. Minimized Maximum of Products Distributed to Any Store](#minimized-maximum-of-products-distributed-to-any-store)
- [2226. Maximum Candies Allocated to K Children](#maximum-candies-allocated-to-k-children)

### IX. Binary Search Patterns / Pattern 56: Binary Search - Find First/Last Occurrence
- [34. Find First and Last Position of Element in Sorted Array](#find-first-and-last-position-of-element-in-sorted-array)
- [658. Find K Closest Elements](#find-k-closest-elements)

### IX. Binary Search Patterns / Pattern 57: Binary Search - Median of Two Sorted Arrays
- [4. Median of Two Sorted Arrays](#median-of-two-sorted-arrays)

### V. Dynamic Programming (DP) Patterns / Pattern 26: DP - 1D Array (Fibonacci Style)
- [70. Climbing Stairs](#climbing-stairs)
- [91. Decode Ways](#decode-ways)
- [198. House Robber](#house-robber)
- [213. House Robber II](#house-robber-ii)
- [337. House Robber III](#house-robber-iii)
- [509. Fibonacci Number](#fibonacci-number)
- [740. Delete and Earn](#delete-and-earn)
- [746. Min Cost Climbing Stairs](#min-cost-climbing-stairs)

### V. Dynamic Programming (DP) Patterns / Pattern 27: DP - 1D Array (Kadane's Algorithm for Max/Min Subarray)
- [53. Maximum Subarray](#maximum-subarray)
- [918. Maximum Sum Circular Subarray](#maximum-sum-circular-subarray)
- [2321. Maximum Score Of Spliced Array](#maximum-score-of-spliced-array)
- [1749. Maximum Absolute Sum of Any Subarray](#maximum-absolute-sum-of-any-subarray)

### V. Dynamic Programming (DP) Patterns / Pattern 28: DP - 1D Array (Coin Change / Unbounded Knapsack Style)
- [322. Coin Change](#coin-change)
- [377. Combination Sum IV](#combination-sum-iv)
- [518. Coin Change II](#coin-change-ii)

### V. Dynamic Programming (DP) Patterns / Pattern 29: DP - 1D Array (0/1 Knapsack Subset Sum Style)
- [416. Partition Equal Subset Sum](#partition-equal-subset-sum)
- [494. Target Sum](#target-sum)

### V. Dynamic Programming (DP) Patterns / Pattern 30: DP - 1D Array (Word Break Style)
- [139. Word Break](#word-break)
- [140. Word Break II](#word-break-ii)

### V. Dynamic Programming (DP) Patterns / Pattern 31: DP - 2D Array (Longest Common Subsequence - LCS)
- [583. Delete Operation for Two Strings](#delete-operation-for-two-strings)
- [1143. Longest Common Subsequence](#longest-common-subsequence)

### V. Dynamic Programming (DP) Patterns / Pattern 32: DP - 2D Array (Edit Distance / Levenshtein Distance)
- [72. Edit Distance](#edit-distance)
- [583. Delete Operation for Two Strings](#delete-operation-for-two-strings)
- [712. Minimum ASCII Delete Sum for Two Strings](#minimum-ascii-delete-sum-for-two-strings)

### V. Dynamic Programming (DP) Patterns / Pattern 33: DP - 2D Array (Unique Paths on Grid)
- [62. Unique Paths](#unique-paths)
- [63. Unique Paths II](#unique-paths-ii)
- [64. Minimum Path Sum](#minimum-path-sum)
- [120. Triangle](#triangle)
- [221. Maximal Square](#maximal-square)
- [931. Minimum Falling Path Sum](#minimum-falling-path-sum)
- [1277. Count Square Submatrices with All Ones](#count-square-submatrices-with-all-ones)

### V. Dynamic Programming (DP) Patterns / Pattern 34: DP - Interval DP
- [312. Burst Balloons](#burst-balloons)
- [546. Remove Boxes](#remove-boxes)

### V. Dynamic Programming (DP) Patterns / Pattern 35: DP - Catalan Numbers
- [95. Unique Binary Search Trees II](#unique-binary-search-trees-ii)
- [96. Unique Binary Search Trees](#unique-binary-search-trees)
- [241. Different Ways to Add Parentheses](#different-ways-to-add-parentheses)

### V. Dynamic Programming (DP) Patterns / Pattern 36: DP - Longest Increasing Subsequence (LIS)
- [300. Longest Increasing Subsequence](#longest-increasing-subsequence)
- [354. Russian Doll Envelopes](#russian-doll-envelopes)
- [1671. Minimum Number of Removals to Make Mountain Array](#minimum-number-of-removals-to-make-mountain-array)
- [2407. Longest Increasing Subsequence II](#longest-increasing-subsequence-ii)

### V. Dynamic Programming (DP) Patterns / Pattern 37: DP - Stock problems
- [121. Best Time to Buy and Sell Stock](#best-time-to-buy-and-sell-stock)
- [122. Best Time to Buy and Sell Stock II](#best-time-to-buy-and-sell-stock-ii)
- [123. Best Time to Buy and Sell Stock III](#best-time-to-buy-and-sell-stock-iii)
- [188.Best Time to Buy and Sell Stock IV](#best-time-to-buy-and-sell-stock-iv)
- [309. Best Time to Buy and Sell Stock with Cooldown](#best-time-to-buy-and-sell-stock-with-cooldown)

### VI. Heap (Priority Queue) Patterns / Pattern 37: Heap - Top K Elements (Selection/Frequency)
- [215. Kth Largest Element in an Array](#kth-largest-element-in-an-array)
- [347. Top K Frequent Elements](#top-k-frequent-elements)
- [451. Sort Characters By Frequency](#sort-characters-by-frequency)
- [506. Relative Ranks](#relative-ranks)
- [703. Kth Largest Element in a Stream](#kth-largest-element-in-a-stream)
- [973. K Closest Points to Origin](#k-closest-points-to-origin)
- [1046. Last Stone Weight](#last-stone-weight)
- [2558. Take Gifts From the Richest Pile](#take-gifts-from-the-richest-pile)

### VI. Heap (Priority Queue) Patterns / Pattern 38: Heap - Two Heaps for Median Finding
- [295. Find Median from Data Stream](#find-median-from-data-stream)
- [1825. Finding MK Average](#finding-mk-average)

### VI. Heap (Priority Queue) Patterns / Pattern 39: Heap - K-way Merge
- [23. Merge k Sorted Lists](#merge-k-sorted-lists)
- [373. Find K Pairs with Smallest Sums](#find-k-pairs-with-smallest-sums)
- [378. Kth Smallest Element in a Sorted Matrix](#kth-smallest-element-in-a-sorted-matrix)
- [632. Smallest Range Covering Elements from K Lists](#smallest-range-covering-elements-from-k-lists)

### VI. Heap (Priority Queue) Patterns / Pattern 40: Heap - Scheduling / Minimum Cost (Greedy with Priority Queue)
- [253. Meeting Rooms II](#meeting-rooms-ii)
- [767. Reorganize String](#reorganize-string)
- [857. Minimum Cost to Hire K Workers](#minimum-cost-to-hire-k-workers)
- [1642. Furthest Building You Can Reach](#furthest-building-you-can-reach)
- [1792. Maximum Average Pass Ratio](#maximum-average-pass-ratio)
- [1834. Single-Threaded CPU](#single-threaded-cpu)
- [1942. The Number of the Smallest Unoccupied Chair](#the-number-of-the-smallest-unoccupied-chair)
- [2402. Meeting Rooms III](#meeting-rooms-iii)

### VII. Backtracking Patterns / Pattern 41: Backtracking - Subsets (Include/Exclude)
- [17. Letter Combinations of a Phone Number](#letter-combinations-of-a-phone-number)
- [77. Combinations](#combinations)
- [78. Subsets](#subsets)
- [90. Subsets II](#subsets-ii)

### VII. Backtracking Patterns / Pattern 42: Backtracking - Permutations
- [31. Next Permutation](#next-permutation)
- [46. Permutations](#permutations)
- [60. Permutation Sequence](#permutation-sequence)

### VII. Backtracking Patterns / Pattern 43: Backtracking - Combination Sum
- [39. Combination Sum](#combination-sum)
- [40. Combination Sum II](#combination-sum-ii)

### VII. Backtracking Patterns / Pattern 44: Backtracking - Parentheses Generation
- [22. Generate Parentheses](#generate-parentheses)
- [301. Remove Invalid Parentheses](#remove-invalid-parentheses)

### VII. Backtracking Patterns / Pattern 45: Backtracking - Word Search / Path Finding in Grid
- [79. Word Search](#word-search)
- [212. Word Search II](#word-search-ii)
- [2018. Check if Word Can Be Placed In Crossword](#check-if-word-can-be-placed-in-crossword)

### VII. Backtracking Patterns / Pattern 46: Backtracking - N-Queens / Constraint Satisfaction
- [37. Sudoku Solver](#sudoku-solver)
- [51. N-Queens](#n-queens)

### VII. Backtracking Patterns / Pattern 47: Backtracking - Palindrome Partitioning
- [131. Palindrome Partitioning](#palindrome-partitioning)
- [132. Palindrome Partitioning II](#palindrome-partitioning-ii)
- [1457. Pseudo-Palindromic Paths in a Binary Tree](#pseudo-palindromic-paths-in-a-binary-tree)

### VIII. Greedy Patterns / Pattern 48: Greedy - Interval Merging/Scheduling
- [56. Merge Intervals](#merge-intervals)
- [57. Insert Interval](#insert-interval)
- [759. Employee Free Time](#employee-free-time)
- [986. Interval List Intersections](#interval-list-intersections)
- [2406. Divide Intervals Into Minimum Number of Groups](#divide-intervals-into-minimum-number-of-groups)

### VIII. Greedy Patterns / Pattern 49: Greedy - Jump Game Reachability/Minimization
- [45. Jump Game II](#jump-game-ii)
- [55. Jump Game](#jump-game)

### VIII. Greedy Patterns / Pattern 50: Greedy - Buy/Sell Stock
- [121. Best Time to Buy and Sell Stock](#best-time-to-buy-and-sell-stock)
- [122. Best Time to Buy and Sell Stock II](#best-time-to-buy-and-sell-stock-ii)

### VIII. Greedy Patterns / Pattern 51: Greedy - Gas Station Circuit
- [134. Gas Station](#gas-station)

### VIII. Greedy Patterns / Pattern 52: Greedy - Task Scheduling (Frequency Based)
- [621. Task Scheduler](#task-scheduler)

### X. Stack Patterns / Pattern 58: Stack - Valid Parentheses Matching
- [20. Valid Parentheses](#valid-parentheses)
- [32. Longest Valid Parentheses](#longest-valid-parentheses)
- [921. Minimum Add to Make Parentheses Valid](#minimum-add-to-make-parentheses-valid)
- [1249. Minimum Remove to Make Valid Parentheses](#minimum-remove-to-make-valid-parentheses)
- [1963. Minimum Number of Swaps to Make the String Balanced](#minimum-number-of-swaps-to-make-the-string-balanced)

### X. Stack Patterns / Pattern 59: Stack - Monotonic Stack
- [402. Remove K Digits](#remove-k-digits)
- [496. Next Greater Element I](#next-greater-element-i)
- [503. Next Greater Element II](#next-greater-element-ii)
- [739. Daily Temperatures](#daily-temperatures)
- [901. Online Stock Span](#online-stock-span)
- [907. Sum of Subarray Minimums](#sum-of-subarray-minimums)
- [962. Maximum Width Ramp](#maximum-width-ramp)
- [1475. Final Prices With a Special Discount in a Shop](#final-prices-with-a-special-discount-in-a-shop)
- [1673. Find the Most Competitive Subsequence](#find-the-most-competitive-subsequence)

### X. Stack Patterns / Pattern 60: Stack - Expression Evaluation (RPN/Infix)
- [150. Evaluate Reverse Polish Notation](#evaluate-reverse-polish-notation)
- [224. Basic Calculator](#basic-calculator)
- [227. Basic Calculator II](#basic-calculator-ii)
- [772. Basic Calculator III](#basic-calculator-iii)

### X. Stack Patterns / Pattern 61: Stack - Simulation / Backtracking Helper
- [71. Simplify Path](#simplify-path)
- [394. Decode String](#decode-string)
- [735. Asteroid Collision](#asteroid-collision)

### X. Stack Patterns / Pattern 62: Stack - Min Stack Design
- [155. Min Stack](#min-stack)

### X. Stack Patterns / Pattern 63: Stack - Largest Rectangle in Histogram
- [84. Largest Rectangle in Histogram](#largest-rectangle-in-histogram)
- [85. Maximal Rectangle](#maximal-rectangle)

### XI. Bit Manipulation Patterns / Pattern 65: Bitwise XOR - Finding Single/Missing Number
- [136. Single Number](#single-number)
- [137. Single Number II](#single-number-ii)
- [268. Missing Number](#missing-number)
- [389. Find the Difference](#find-the-difference)

### XI. Bit Manipulation Patterns / Pattern 66: Bitwise AND - Counting Set Bits (Hamming Weight)
- [191. Number of 1 Bits](#number-of-1-bits)

### XI. Bit Manipulation Patterns / Pattern 67: Bitwise DP - Counting Bits Optimization
- [338. Counting Bits](#counting-bits)

### XI. Bit Manipulation Patterns / Pattern 68: Bitwise Operations - Power of Two/Four Check
- [231. Power of Two](#power-of-two)
- [342. Power of Four](#power-of-four)

### XII. Linked List Manipulation Patterns / Pattern 69: Linked List - In-place Reversal
- [83. Remove Duplicates from Sorted List](#remove-duplicates-from-sorted-list)
- [92. Reverse Linked List II](#reverse-linked-list-ii)
- [206. Reverse Linked List](#reverse-linked-list)
- [25. Reverse Nodes in k-Group](#reverse-nodes-in-k-group)
- [234. Palindrome Linked List](#palindrome-linked-list)
- [82. Remove Duplicates from Sorted List II](#remove-duplicates-from-sorted-list-ii)

### XII. Linked List Manipulation Patterns / Pattern 70: Linked List - Merging Two Sorted Lists
- [21. Merge Two Sorted Lists](#merge-two-sorted-lists)

### XII. Linked List Manipulation Patterns / Pattern 71: Linked List - Addition of Numbers
- [2. Add Two Numbers](#add-two-numbers)
- [369. Plus One Linked List](#plus-one-linked-list)

### XII. Linked List Manipulation Patterns / Pattern 72: Linked List - Intersection Detection
- [160. Intersection of Two Linked Lists](#intersection-of-two-linked-lists)

### XII. Linked List Manipulation Patterns / Pattern 73: Linked List - Reordering / Partitioning
- [24. Swap Nodes in Pairs](#swap-nodes-in-pairs)
- [61. Rotate List](#rotate-list)
- [86. Partition List](#partition-list)
- [143. Reorder List](#reorder-list)
- [328. Odd Even Linked List](#odd-even-linked-list)

### XIII. Array/Matrix Manipulation Patterns / Pattern 74: Array/Matrix - In-place Rotation
- [48. Rotate Image](#rotate-image)
- [189. Rotate Array](#rotate-array)

### XIII. Array/Matrix Manipulation Patterns / Pattern 75: Array/Matrix - Spiral Traversal
- [54. Spiral Matrix](#spiral-matrix)
- [885. Spiral Matrix III](#spiral-matrix-iii)
- [2326. Spiral Matrix IV](#spiral-matrix-iv)

### XIII. Array/Matrix Manipulation Patterns / Pattern 76: Array/Matrix - Set Matrix Zeroes (In-place Marking)
- [73. Set Matrix Zeroes](#set-matrix-zeroes)

### XIII. Array/Matrix Manipulation Patterns / Pattern 77: Array - Product Except Self (Prefix/Suffix Products)
- [238. Product of Array Except Self](#product-of-array-except-self)

### XIII. Array/Matrix Manipulation Patterns / Pattern 78: Array - Plus One (Handling Carry)
- [66. Plus One](#plus-one)

### XIII. Array/Matrix Manipulation Patterns / Pattern 79: Array - Merge Sorted Array (In-place from End)
- [88. Merge Sorted Array](#merge-sorted-array)

### XIII. Array/Matrix Manipulation Patterns / Pattern 80: Array - Cyclic Sort
- [41. First Missing Positive](#first-missing-positive)
- [268. Missing Number](#missing-number)
- [287. Find the Duplicate Number](#find-the-duplicate-number)
- [442. Find All Duplicates in an Array](#find-all-duplicates-in-an-array)
- [448. Find All Numbers Disappeared in an Array](#find-all-numbers-disappeared-in-an-array)

### XIII. Array/Matrix Manipulation Patterns / Pattern 81: Array - Kadane's Variant for Maximum Product
- [152. Maximum Product Subarray](#maximum-product-subarray)

### XIV. String Manipulation Patterns / Pattern 82: String - Palindrome Check (Two Pointers / Reverse)
- [9. Palindrome Number](#palindrome-number)
- [125. Valid Palindrome](#valid-palindrome)
- [680. Valid Palindrome II](#valid-palindrome-ii)

### XIV. String Manipulation Patterns / Pattern 83: String - Anagram Check (Frequency Count/Sort)
- [49. Group Anagrams](#group-anagrams)
- [242. Valid Anagram](#valid-anagram)

### XIV. String Manipulation Patterns / Pattern 84: String - Roman to Integer Conversion
- [13. Roman to Integer](#roman-to-integer)

### XIV. String Manipulation Patterns / Pattern 85: String - String to Integer (atoi)
- [8. String to Integer (atoi)](#string-to-integer-atoi)

### XIV. String Manipulation Patterns / Pattern 86: String - Multiply Strings (Manual Simulation)
- [43. Multiply Strings](#multiply-strings)

### XIV. String Manipulation Patterns / Pattern 87: String Matching - Naive / KMP / Rabin-Karp
- [28. Find the Index of the First Occurrence in a String](#find-the-index-of-the-first-occurrence-in-a-string)
- [214. Shortest Palindrome](#shortest-palindrome)
- [686. Repeated String Match](#repeated-string-match)
- [796. Rotate String](#rotate-string)
- [3008. Find Beautiful Indices in the Given Array II](#find-beautiful-indices-in-the-given-array-ii)

### XIV. String Manipulation Patterns / Pattern 88: String - Repeated Substring Pattern Detection
- [459. Repeated Substring Pattern](#repeated-substring-pattern)

### XV. Design Patterns / Pattern 89: Design (General/Specific)
- [146. LRU Cache](#lru-cache)
- [155. Min Stack](#min-stack)
- [208. Implement Trie (Prefix Tree)](#implement-trie-prefix-tree)
- [211. Design Add and Search Words Data Structure](#design-add-and-search-words-data-structure)
- [225. Implement Stack using Queues](#implement-stack-using-queues)
- [232. Implement Queue using Stacks](#implement-queue-using-stacks)
- [251. Flatten 2D Vector](#flatten-2d-vector)
- [271. Encode and Decode Strings](#encode-and-decode-strings)
- [295. Find Median from Data Stream](#find-median-from-data-stream)
- [341. Flatten Nested List Iterator](#flatten-nested-list-iterator)
- [346. Moving Average from Data Stream](#moving-average-from-data-stream)
- [353. Design Snake Game](#design-snake-game)
- [359. Logger Rate Limiter](#logger-rate-limiter)
- [362. Design Hit Counter](#design-hit-counter)
- [379. Design Phone Directory](#design-phone-directory)
- [380. Insert Delete GetRandom O(1)](#insert-delete-getrandom-o1)
- [432. All O`one Data Structure](#all-oone-data-structure)
- [460. LFU Cache](#lfu-cache)
- [604. Design Compressed String Iterator](#design-compressed-string-iterator)
- [622. Design Circular Queue](#design-circular-queue)
- [641. Design Circular Deque](#design-circular-deque)
- [642. Design Search Autocomplete System](#design-search-autocomplete-system)
- [706. Design HashMap](#design-hashmap)
- [715. Range Module](#range-module)
- [900. RLE Iterator](#rle-iterator)
- [981. Time Based Key-Value Store](#time-based-key-value-store)
- [1146. Snapshot Array](#snapshot-array)
- [1348. Tweet Counts Per Frequency](#tweet-counts-per-frequency)
- [1352. Product of the Last K Numbers](#product-of-the-last-k-numbers)
- [1381. Design a Stack With Increment Operation](#design-a-stack-with-increment-operation)
- [1756. Design Most Recently Used Queue](#design-most-recently-used-queue)
- [2013. Detect Squares](#detect-squares)
- [2034. Stock Price Fluctuation](#stock-price-fluctuation)
- [2296. Design a Text Editor](#design-a-text-editor)
- [2336. Smallest Number in Infinite Set](#smallest-number-in-infinite-set)

---

---
<a id="container-with-most-water"></a>
### **11. Container With Most Water**
**Link to Problem:** [https://leetcode.com/problems/container-with-most-water/](https://leetcode.com/problems/container-with-most-water/)

#### **1. Problem Statement**
Given an array of non-negative integers `height`, where each integer represents the height of a vertical line at a specific coordinate, the goal is to find the two lines that, along with the x-axis, create a container with the largest possible water capacity. The output should be this maximum area.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to consider every possible pair of vertical lines and calculate the area of the container they form. We can use a nested loop structure: the outer loop picks the first line (`i`), and the inner loop iterates through all subsequent lines (`j`) to form a pair. For each pair, we calculate the area and update a variable that tracks the maximum area found so far.

The area for any pair of lines `i` and `j` is determined by the shorter of the two lines (as water would spill over the shorter side) and the distance between them.
`Area = width * height = (j - i) * min(height[i], height[j])`

**Python Code:**
```python
from typing import List

class Solution:
    def maxArea(self, height: List[int]) -> int:
        max_area = 0
        n = len(height)

        # The outer loop fixes the left line of the container.
        for i in range(n):
            # The inner loop tries every possible right line for the fixed left line.
            for j in range(i + 1, n):
                # The width is the distance between the two lines.
                width = j - i
                
                # The height of the container is limited by the shorter of the two lines.
                container_height = min(height[i], height[j])
                
                # Calculate the area for the current pair of lines.
                current_area = width * container_height
                
                # Update the maximum area found so far.
                max_area = max(max_area, current_area)
                
        return max_area

```
**Complexity Analysis:**

*   **Time Complexity: `O(N^2)`**
    This is due to the nested loops. For an array of size `N`, the outer loop runs `N` times, and the inner loop runs approximately `N` times for each outer iteration, leading to a quadratic time complexity.

*   **Space Complexity: `O(1)`**
    The algorithm uses a constant amount of extra space, as we only need a few variables to store the max area and loop indices.

### **3. Optimized Approach: Two Pointers - Converging**
**Intuition:**
The brute-force approach is inefficient because it considers many pairs that are guaranteed to be suboptimal. We can do much better by being strategic. The key insight is that the area is constrained by both the width and the height. To maximize the area, we want to maximize both.

This leads to the **Two Pointers** pattern. We can start with the widest possible container by placing one pointer (`left`) at the beginning of the array and another (`right`) at the very end. This configuration has the maximum possible width.

Now, how do we find a potentially larger area? We can't increase the width any further, so our only hope is to find a taller pair of lines. The current container's height is limited by the shorter of `height[left]` and `height[right]`.

Let's say `height[left]` is shorter than `height[right]`. If we move the `right` pointer inward, the width will decrease, and the new height will still be limited by `height[left]`. The new area will *definitively* be smaller. Therefore, there is no benefit in moving the pointer of the taller line.

The only logical move is to move the pointer of the *shorter* line inward. By doing this, we sacrifice a small amount of width, but we gain the *possibility* of finding a much taller line, which could lead to a larger overall area. We repeat this processâ€”calculate the area, and move the shorter pointer inwardâ€”until the two pointers meet.

**Example:** `height = [1, 8, 6, 2, 5, 4, 8, 3, 7]`

1.  `left = 0`, `right = 8`. `h[l]=1`, `h[r]=7`. Width is 8.
    Area = `8 * min(1, 7) = 8`. Max area is 8.
    `h[l]` is shorter, so move `left` pointer: `left++`.
2.  `left = 1`, `right = 8`. `h[l]=8`, `h[r]=7`. Width is 7.
    Area = `7 * min(8, 7) = 49`. Max area is 49.
    `h[r]` is shorter, so move `right` pointer: `right--`.
3.  `left = 1`, `right = 7`. `h[l]=8`, `h[r]=3`. Width is 6.
    Area = `6 * min(8, 3) = 18`. Max area is still 49.
    `h[r]` is shorter, so move `right` pointer: `right--`.
4.  ...and so on, until `left` and `right` cross.

**Python Code:**
```python
from typing import List

class Solution:
    def maxArea(self, height: List[int]) -> int:
        max_area = 0
        # Initialize two pointers at opposite ends of the array.
        left, right = 0, len(height) - 1
        
        # Loop until the pointers converge.
        while left < right:
            # Calculate the width of the current container.
            width = right - left
            
            # The height is limited by the shorter of the two lines.
            container_height = min(height[left], height[right])
            
            # Calculate the area and update the maximum.
            current_area = width * container_height
            max_area = max(max_area, current_area)
            
            # This is the core logic of the pattern.
            # We move the pointer that points to the shorter line.
            # This is because the shorter line is the limiting factor, and moving it
            # gives us a chance to find a taller line, which could increase the area.
            if height[left] < height[right]:
                left += 1  # Move the left pointer inward.
            else:
                right -= 1 # Move the right pointer inward.
                
        return max_area
```
**Complexity Analysis:**

*   **Time Complexity: `O(N)`**
    This is a significant improvement. The `left` pointer only moves from left to right, and the `right` pointer only moves from right to left. In each step of the `while` loop, one of the pointers moves. This means we will traverse the array only once, resulting in linear time complexity.

*   **Space Complexity: `O(1)`**
    Similar to the brute-force approach, we only use a constant amount of extra space for the pointers and the max area variable.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging** pattern. While often associated with sorted arrays (like finding a target sum), its core principle is applicable here in a more creative way.

The key signals that point to this pattern are:

1.  **Seeking a Pairwise Optimal Value:** The problem asks for the best *pair* of lines out of many possibilities to maximize a value (area).
2.  **Value Depends on Distance:** The area calculation depends not only on the values at the pointers (`height[i]`, `height[j]`) but also on the distance between them (`j - i`).
3.  **A "Greedy" Pruning Strategy:** The crucial characteristic is the ability to make a "greedy" decision to discard a large part of the search space. By starting at the maximum width, we can definitively say that to get a larger area, we *must* find a taller height. The only way to potentially achieve this is by moving the pointer of the current *shorter* line. This allows us to eliminate the shorter line from all future considerations, efficiently "converging" towards the optimal solution in a single pass.

---

---
<a id="3sum"></a>
### **15. 3Sum**
**Link to Problem:** [https://leetcode.com/problems/3sum/](https://leetcode.com/problems/3sum/)

#### **1. Problem Statement**
Given an integer array `nums`, the task is to find all unique triplets `[nums[i], nums[j], nums[k]]` such that `i`, `j`, and `k` are distinct indices and the sum of the three elements is equal to zero. The final output should not contain any duplicate triplets.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to check every possible combination of three distinct numbers in the array. We can use three nested loops to iterate through all possible triplets. The first loop picks the first number, the second loop picks a second number after the first, and the third loop picks a third number after the second. Inside the innermost loop, we check if their sum is zero.

A key challenge is handling duplicate triplets (e.g., `[-1, 0, 1]` is the same as `[0, 1, -1]`). To solve this, we can sort each valid triplet before storing it. Using a `set` to store the sorted triplets ensures that we only keep the unique ones.

**Python Code:**
```python
def threeSum_brute_force(nums: list[int]) -> list[list[int]]:
    n = len(nums)
    if n < 3:
        return []
    
    # A set is used to automatically handle duplicate triplets.
    # We store tuples because lists are not hashable.
    result_set = set()

    # Three nested loops to check every possible combination of three numbers.
    for i in range(n - 2):
        for j in range(i + 1, n - 1):
            for k in range(j + 1, n):
                # Check if the sum is zero
                if nums[i] + nums[j] + nums[k] == 0:
                    # Sort the triplet to handle permutations as duplicates
                    # e.g., [-1, 0, 1] and [1, 0, -1] should be treated as the same.
                    triplet = tuple(sorted([nums[i], nums[j], nums[k]]))
                    result_set.add(triplet)
    
    # Convert the set of tuples back to a list of lists.
    return [list(triplet) for triplet in result_set]

```
**Complexity Analysis:**

*   **Time Complexity: O(N^3)**
    This is due to the three nested loops, each iterating through the array. For an array of size N, the number of triplet combinations is roughly N * N * N.

*   **Space Complexity: O(K)**
    Where K is the number of unique triplets found. In the worst case, the `result_set` could store a significant number of triplets, potentially proportional to O(N^2) in some edge cases.

### **3. Optimized Approach: Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)**
**Intuition:**
The O(N^3) complexity of the brute-force approach is too slow for typical constraints. We can improve this by reducing the problem to a familiar, simpler one. If we fix one number, `nums[i]`, our equation `a + b + c = 0` becomes `b + c = -a`. This is now the classic "2Sum" problem: find two numbers in the rest of the array that sum up to a specific target (`-nums[i]`).

The key to solving the 2Sum subproblem efficiently is to first **sort the array**. Once the array is sorted, we can use the **Converging Two Pointers** pattern.

Here's the step-by-step logic:
1.  **Sort the Input Array:** Sorting is crucial. It allows us to easily skip duplicate values and use the two-pointer technique.
2.  **Iterate with One Pointer:** Loop through the sorted array with a single index `i`. This `nums[i]` will be the first element of our potential triplet.
3.  **Handle Duplicates:** If `nums[i]` is the same as `nums[i-1]`, we've already processed all possible triplets starting with this value, so we can skip it to avoid duplicate results.
4.  **Set Up Two Pointers:** For each `nums[i]`, we define our target as `target = -nums[i]`. We then initialize a `left` pointer to `i + 1` and a `right` pointer to the end of the array (`n - 1`).
5.  **Converge Pointers:** We move the `left` and `right` pointers inward until they meet:
    *   If `nums[left] + nums[right] == target`, we have found a valid triplet: `[nums[i], nums[left], nums[right]]`. We add it to our results. Then, to avoid duplicates, we move the `left` pointer forward past any identical values and the `right` pointer backward past any identical values.
    *   If `nums[left] + nums[right] < target`, the sum is too small. To increase the sum, we must move the `left` pointer to the right (`left += 1`).
    *   If `nums[left] + nums[right] > target`, the sum is too large. To decrease the sum, we must move the `right` pointer to the left (`right -= 1`).

**Example:** `nums = [-1, 0, 1, 2, -1, -4]`
1.  **Sort:** `nums = [-4, -1, -1, 0, 1, 2]`
2.  **Outer loop `i = 0`**, `nums[i] = -4`. Target = 4.
    *   `left=1`, `right=5`. `nums[left]+nums[right] = -1+2 = 1`. Too small. `left++`.
    *   `left=2`, `right=5`. `nums[left]+nums[right] = -1+2 = 1`. Too small. `left++`.
    *   ...pointers converge, no solution found.
3.  **Outer loop `i = 1`**, `nums[i] = -1`. Target = 1.
    *   `left=2`, `right=5`. `nums[left]+nums[right] = -1+2 = 1`. **Found!** Result: `[[-1, -1, 2]]`.
    *   Move pointers past duplicates. `left` becomes 3, `right` becomes 4.
    *   `left=3`, `right=4`. `nums[left]+nums[right] = 0+1 = 1`. **Found!** Result: `[[-1, -1, 2], [-1, 0, 1]]`.
    *   Move pointers. They now cross. End inner loop.
4.  **Outer loop `i = 2`**, `nums[i] = -1`. This is a duplicate of `nums[i-1]`, so we `continue`.
5.  ...and so on.

**Python Code:**
```python
def threeSum(nums: list[int]) -> list[list[int]]:
    n = len(nums)
    result = []
    
    # Sorting the array is the key to using the two-pointer approach.
    nums.sort()
    
    # Outer loop fixes the first element of the triplet.
    for i in range(n - 2):
        # Optimization: If the current number is positive, the sum can't be zero
        # since the array is sorted and all subsequent numbers will also be positive.
        if nums[i] > 0:
            break
            
        # Skip duplicate values for the first element to avoid duplicate triplets.
        if i > 0 and nums[i] == nums[i - 1]:
            continue
            
        # Set up the two pointers for the rest of the array.
        left, right = i + 1, n - 1
        
        # This is the core of the Two Pointers pattern.
        while left < right:
            current_sum = nums[i] + nums[left] + nums[right]
            
            if current_sum == 0:
                # Found a valid triplet.
                result.append([nums[i], nums[left], nums[right]])
                
                # --- Handle Duplicates for the other two elements ---
                # Move left pointer forward as long as it's a duplicate.
                while left < right and nums[left] == nums[left + 1]:
                    left += 1
                # Move right pointer backward as long as it's a duplicate.
                while left < right and nums[right] == nums[right - 1]:
                    right -= 1
                    
                # Move pointers to the next unique elements.
                left += 1
                right -= 1
            elif current_sum < 0:
                # The sum is too small, so we need a larger number.
                # Move the left pointer inward.
                left += 1
            else: # current_sum > 0
                # The sum is too big, so we need a smaller number.
                # Move the right pointer inward.
                right -= 1
                
    return result

```
**Complexity Analysis:**

*   **Time Complexity: O(N^2)**
    The initial sort takes O(N log N). The main loop runs N times. Inside it, the two-pointer `while` loop traverses the remaining part of the array, which takes O(N) time in the worst case for each `i`. This gives us O(N log N + N^2), which simplifies to O(N^2) as it's the dominant term.

*   **Space Complexity: O(log N) to O(N)**
    This depends on the implementation of the sorting algorithm used. In Python, Timsort uses O(N) auxiliary space in the worst case. If we ignore the space required for the output list `result`, the space complexity is determined by the sort.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging** pattern, elevated by nesting it within a primary loop. The signals that point to this pattern are:

1.  **Sorted Array:** The pattern relies fundamentally on the array being sorted. The need to find combinations often implies that sorting can structure the search space, which is the first hint.
2.  **Target Sum:** The problem asks for a combination of elements (a triplet) that sum to a specific target (zero). This "find a sum" requirement is a classic use case for two pointers.
3.  **Efficient Search:** The brute-force O(N^3) is clearly inefficient. After sorting and fixing one element `a`, the converging pointers (`left` starting from the beginning and `right` from the end) provide an O(N) method to search for the remaining pair `b` and `c`. The logic is simple and powerful: if the current sum is too small, only increasing the `left` pointer can help; if it's too large, only decreasing the `right` pointer can help. This systematic elimination of the search space is the hallmark of the pattern.

By recognizing that `3Sum` can be broken down into `N` instances of the `2Sum on a Sorted Array` problem, you can immediately identify it as a prime candidate for the Two Pointers pattern.

---

---
<a id="3sum-closest"></a>
### **16. 3Sum Closest**
**Link to Problem:** [https://leetcode.com/problems/3sum-closest/](https://leetcode.com/problems/3sum-closest/)

#### **1. Problem Statement**
Given an array of integers `nums` and an integer `target`, the task is to find three integers in `nums` whose sum is closest to the `target`. You must return the sum of these three integers. It is guaranteed that each input will have exactly one solution.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to test every possible combination of three distinct numbers from the array. We can use three nested loops to iterate through all unique triplets. For each triplet, we calculate its sum and compare its absolute difference from the `target` with the smallest difference found so far. If the current triplet's sum is closer, we update our result.

**Python Code:**
```python
import math

class Solution:
    def threeSumClosest(self, nums: list[int], target: int) -> int:
        # Initialize with a very large difference to ensure the first sum is always closer.
        min_diff = math.inf
        closest_sum = 0
        n = len(nums)

        # The first loop selects the first number of the triplet.
        for i in range(n - 2):
            # The second loop selects the second number, starting after the first.
            for j in range(i + 1, n - 1):
                # The third loop selects the third number, starting after the second.
                for k in range(j + 1, n):
                    current_sum = nums[i] + nums[j] + nums[k]
                    current_diff = abs(target - current_sum)

                    # If this triplet's sum is closer to the target, update our result.
                    if current_diff < min_diff:
                        min_diff = current_diff
                        closest_sum = current_sum
        
        return closest_sum

```
**Complexity Analysis:**

*   **Time Complexity: O(NÂ³)**
    This is due to the three nested loops, each of which can iterate up to N times, where N is the number of elements in `nums`. This approach is very slow for large inputs.

*   **Space Complexity: O(1)**
    We only use a few variables (`min_diff`, `closest_sum`, loop counters) for storage, regardless of the input size.

#### **3. Optimized Approach: [Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)]**
**Intuition:**
The brute-force O(NÂ³) complexity is inefficient. We can significantly improve this by first sorting the array. Sorting allows us to use a more methodical approach to finding the other two numbers once we've fixed one.

This problem can be reduced to a "2Sum Closest" problem. We iterate through the array with a single loop, fixing one number at a time (`nums[i]`). For each `nums[i]`, our goal is to find two other numbers in the rest of the array (`nums[i+1:]`) whose sum, combined with `nums[i]`, is as close to the `target` as possible.

This is where the **Converging Two Pointers** pattern excels. For each `nums[i]`, we set up two pointers in the remaining part of the array: a `left` pointer at `i + 1` and a `right` pointer at the end of the array. We then calculate the sum of the triplet `(nums[i], nums[left], nums[right])`.

Based on this sum, we can intelligently move the pointers:
*   If `current_sum` < `target`, we need a larger sum. Since the array is sorted, we move the `left` pointer one step to the right to include a larger number.
*   If `current_sum` > `target`, we need a smaller sum. We move the `right` pointer one step to the left to include a smaller number.
*   If `current_sum` == `target`, we've found a sum with a difference of 0, which is the best possible result. We can return this sum immediately.

We repeat this process, converging the `left` and `right` pointers, until they cross.

**Example:** `nums = [-1, 2, 1, -4]`, `target = 1`
1.  **Sort `nums`:** `[-4, -1, 1, 2]`
2.  Initialize `closest_sum` with the sum of the first three elements: `(-4) + (-1) + 1 = -4`. The initial minimum difference is `abs(1 - (-4)) = 5`.
3.  **Outer loop `i = 0` (`nums[i] = -4`):**
    *   `left = 1`, `right = 3`. Triplet: `(-4, -1, 2)`. Sum = `-3`.
    *   Difference `abs(1 - (-3)) = 4`. This is better than 5, so `closest_sum` becomes `-3`.
    *   Sum `-3` < `target` 1, so we need a larger sum. Increment `left`.
    *   `left = 2`, `right = 3`. Triplet: `(-4, 1, 2)`. Sum = `-1`.
    *   Difference `abs(1 - (-1)) = 2`. This is better than 4, so `closest_sum` becomes `-1`.
    *   Sum `-1` < `target` 1, so increment `left`. Now `left` equals `right`, ending the inner loop.
4.  **Outer loop `i = 1` (`nums[i] = -1`):**
    *   `left = 2`, `right = 3`. Triplet: `(-1, 1, 2)`. Sum = `2`.
    *   Difference `abs(1 - 2) = 1`. This is better than 2, so `closest_sum` becomes `2`.
    *   Sum `2` > `target` 1, so we need a smaller sum. Decrement `right`. Now `left` equals `right`, ending the inner loop.
5.  The outer loop finishes. The final `closest_sum` is **2**.

**Python Code:**
```python
import math

class Solution:
    def threeSumClosest(self, nums: list[int], target: int) -> int:
        # Sorting is crucial for the two-pointer approach.
        nums.sort()
        
        min_diff = math.inf
        closest_sum = 0
        n = len(nums)

        # Main loop to fix the first element of the triplet.
        for i in range(n - 2):
            # Initialize two pointers for the rest of the array.
            left = i + 1
            right = n - 1

            # The two pointers converge towards each other.
            while left < right:
                current_sum = nums[i] + nums[left] + nums[right]
                
                # Check if the current sum is closer to the target.
                current_diff = abs(target - current_sum)
                if current_diff < min_diff:
                    min_diff = current_diff
                    closest_sum = current_sum
                
                # --- Core logic of the Two Pointers pattern ---
                # Move pointers based on the comparison with the target.
                if current_sum < target:
                    # If the sum is too small, move the left pointer to a larger value.
                    left += 1
                elif current_sum > target:
                    # If the sum is too large, move the right pointer to a smaller value.
                    right -= 1
                else:
                    # If the sum is exactly the target, we've found the best possible answer.
                    return target
        
        return closest_sum

```
**Complexity Analysis:**

*   **Time Complexity: O(NÂ²)**
    The initial sort costs O(N log N). The main logic consists of a `for` loop that runs N times, and inside it, a `while` loop with two pointers that traverse the rest of the array. This inner traversal takes O(N) time. Therefore, the total time complexity is O(N log N + NÂ²), which simplifies to O(NÂ²).

*   **Space Complexity: O(log N) to O(N)**
    This depends on the space complexity of the sorting algorithm used by the Python environment. Timsort, Python's default, can take up to O(N) space in the worst case. If we disregard the space for sorting, the algorithm itself uses O(1) extra space.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging** pattern for several key reasons:

1.  **Sorted Array Prerequisite:** The pattern's effectiveness hinges on the array being sorted. Sorting allows us to make a directional decision: if our sum is too small, we *know* we must move the left pointer rightward to a larger number. This sorted property is the primary signal to consider a two-pointer approach.

2.  **Finding a Combination with a Target Property:** The core task is to find a triplet (a specific combination of elements) that satisfies a condition related to a `target`. The two-pointer pattern is highly efficient for searching for pairs or triplets that meet sum-related criteria in a sorted array.

3.  **Reduction of Search Space:** By fixing one element with an outer loop, the problem is simplified to finding the best *pair* in a sub-array. The two pointers, starting at opposite ends and converging, systematically and efficiently eliminate possibilities. Instead of a brute-force O(NÂ²) check for the pair, this pattern finds the best pair in just O(N) time, leading to the overall O(NÂ²) solution for the 3Sum problem.

Whenever you encounter a problem that involves finding triplets, quadruplets, or pairs in an array that sum up to a target value, and constraints allow for an O(NÂ²) or O(NÂ³) solution, your first thought should be to **sort the array and apply the two-pointers pattern**. This problem extends the classic "2Sum" pattern by wrapping it in an outer loop, a very common and powerful technique.

---

---
<a id="4sum"></a>
### **18. 4Sum**
**Link to Problem:** [https://leetcode.com/problems/4sum/](https://leetcode.com/problems/4sum/)

#### **1. Problem Statement**
Given an array `nums` of `n` integers and a target integer `target`, the task is to find all unique quadruplets `[nums[a], nums[b], nums[c], nums[d]]` such that their sum equals the `target`. The solution set must not contain duplicate quadruplets.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to test every possible combination of four distinct numbers from the array. We can achieve this by using four nested loops, where each loop iterates through the array to pick one number. The outer loop picks the first number, the second loop picks the second number from the remaining part of the array, and so on.

Inside the innermost loop, we check if the sum of the four selected numbers equals the `target`. A major challenge is handling duplicate quadruplets in the output. For example, if the input is `[2, 2, 2, 2, 2]` and the target is 8, `[2, 2, 2, 2]` is the only unique answer. To solve this, we can sort each valid quadruplet before adding it to a `set` data structure, which automatically handles uniqueness.

**Python Code:**
```python
def fourSum_brute_force(nums: list[int], target: int) -> list[list[int]]:
    n = len(nums)
    if n < 4:
        return []
    
    # Use a set of tuples to automatically handle uniqueness of quadruplets.
    result_set = set()
    
    # Four nested loops to check every combination of four numbers.
    for i in range(n):
        for j in range(i + 1, n):
            for k in range(j + 1, n):
                for l in range(k + 1, n):
                    # If the sum of the four numbers equals the target...
                    if nums[i] + nums[j] + nums[k] + nums[l] == target:
                        # ...create a quadruplet, sort it to handle permutation duplicates
                        # (e.g., [1, 2] and [2, 1] become the same),
                        # and add it to the set as a tuple.
                        quadruplet = sorted([nums[i], nums[j], nums[k], nums[l]])
                        result_set.add(tuple(quadruplet))
                        
    # Convert the set of tuples back to a list of lists.
    return [list(q) for q in result_set]

```
**Complexity Analysis:**

*   **Time Complexity: O(Nâ´)**
    This is due to the four nested loops, each iterating up to `N` times, where `N` is the number of elements in `nums`. The sorting and set insertion inside the loop take negligible time compared to the nested loops.

*   **Space Complexity: O(M)**
    Where `M` is the number of unique quadruplets found. The space is used to store the `result_set`. In the worst case, the number of quadruplets can be large.

---
### **3. Optimized Approach: [Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)]**
**Intuition:**
The O(Nâ´) complexity of the brute-force approach is too slow for larger inputs. We can significantly improve this by reducing the problem's dimensionality. The core idea is to generalize the solution for 2Sum and 3Sum. We can fix two numbers with two outer loops and then use the **Two Pointers - Converging** pattern to find the remaining two numbers.

Here's the step-by-step logic:
1.  **Sort the Array:** First, sort the input array `nums`. This is a critical prerequisite for the two-pointer pattern to work and also makes it much easier to handle duplicates.
2.  **Outer Loops (Fixing Two Numbers):** Iterate through the array with a first loop for the first number (`i`) and a nested second loop for the second number (`j`).
3.  **Reduce to 2Sum:** For each pair `(nums[i], nums[j])`, our problem becomes finding two other numbers in the rest of the array that sum up to `new_target = target - nums[i] - nums[j]`. This is now a classic 2Sum problem on a sorted subarray.
4.  **Converging Pointers:** We apply the two-pointer pattern on the subarray starting from `j + 1`.
    *   Initialize a `left` pointer to `j + 1`.
    *   Initialize a `right` pointer to the end of the array, `n - 1`.
    *   While `left < right`, calculate `current_sum = nums[left] + nums[right]`.
        *   If `current_sum == new_target`, we've found a valid quadruplet: `[nums[i], nums[j], nums[left], nums[right]]`. We add it to our results.
        *   If `current_sum < new_target`, we need a larger sum. We move the `left` pointer one step to the right (`left += 1`) to include a larger number.
        *   If `current_sum > new_target`, we need a smaller sum. We move the `right` pointer one step to the left (`right -= 1`) to include a smaller number.
5.  **Skip Duplicates:** Since the array is sorted, duplicates are grouped together.
    *   To avoid duplicate quadruplets, if we find a valid solution, we must advance the `left` and `right` pointers past all identical subsequent elements.
    *   Similarly, in the outer loops, we must skip subsequent identical elements for `i` and `j` to avoid processing the same starting pairs.

**Example Walkthrough:**
`nums = [1, 0, -1, 0, -2, 2]`, `target = 0`
1.  **Sort:** `nums` becomes `[-2, -1, 0, 0, 1, 2]`.
2.  **Outer loop `i = 0`**, `nums[i] = -2`.
3.  **Inner loop `j = 1`**, `nums[j] = -1`. `new_target = 0 - (-2) - (-1) = 3`.
    *   We now need to find two numbers that sum to 3 in `[0, 0, 1, 2]`.
    *   `left = 2` (`nums[left] = 0`), `right = 5` (`nums[right] = 2`).
    *   `current_sum = 0 + 2 = 2`. Since `2 < 3`, we need a larger sum. `left++`.
    *   `left = 3` (`nums[left] = 0`), `right = 5` (`nums[right] = 2`).
    *   `current_sum = 0 + 2 = 2`. Since `2 < 3`, `left++`.
    *   `left = 4` (`nums[left] = 1`), `right = 5` (`nums[right] = 2`).
    *   `current_sum = 1 + 2 = 3`. This matches `new_target`. We found a quadruplet: `[-2, -1, 1, 2]`. Add to results.
    *   Increment `left` and decrement `right`. The pointers cross, so this inner 2Sum loop ends.
4.  The loops for `i` and `j` continue, skipping duplicates along the way, until all combinations are explored.

**Python Code:**
```python
def fourSum(nums: list[int], target: int) -> list[list[int]]:
    n = len(nums)
    if n < 4:
        return []
    
    # Step 1: Sort the array. This is crucial for the two-pointer approach and for skipping duplicates.
    nums.sort()
    
    result = []
    
    # Step 2: Outer loop to fix the first number of the quadruplet.
    for i in range(n - 3):
        # Skip duplicates for the first number.
        if i > 0 and nums[i] == nums[i-1]:
            continue
            
        # Step 3: Inner loop to fix the second number.
        for j in range(i + 1, n - 2):
            # Skip duplicates for the second number.
            if j > i + 1 and nums[j] == nums[j-1]:
                continue
            
            # Step 4: Reduce to a 2Sum problem and apply the converging two-pointer pattern.
            left = j + 1
            right = n - 1
            
            while left < right:
                current_sum = nums[i] + nums[j] + nums[left] + nums[right]
                
                if current_sum == target:
                    # Found a valid quadruplet.
                    result.append([nums[i], nums[j], nums[left], nums[right]])
                    
                    # Pointer moves inward to search for the next unique pair.
                    left += 1
                    right -= 1
                    
                    # Step 5: Skip any duplicates for the third and fourth numbers.
                    while left < right and nums[left] == nums[left - 1]:
                        left += 1
                    while left < right and nums[right] == nums[right + 1]:
                        right -= 1
                        
                elif current_sum < target:
                    # The sum is too small, we need a larger number.
                    # The left pointer moves inward (to the right).
                    left += 1
                else: # current_sum > target
                    # The sum is too large, we need a smaller number.
                    # The right pointer moves inward (to the left).
                    right -= 1
                    
    return result

```
**Complexity Analysis:**

*   **Time Complexity: O(NÂ³)**
    The initial sort takes O(N log N). The main work is done in the nested loops. The outer loop runs `N` times, the second loop runs `N` times, and the inner `while` loop (the two-pointer part) runs in O(N) time. This results in a total complexity of O(N * N * N) = O(NÂ³). The O(NÂ³) term dominates the initial sort.

*   **Space Complexity: O(log N) to O(N)**
    This depends on the space used by the sorting algorithm. Python's Timsort can use up to O(N) space in the worst case. If we ignore the space required for the output list, the auxiliary space complexity is determined by the sort.

---
#### **4. Pattern Connection**
This problem is a classic extension of the `K-Sum` problem category and perfectly demonstrates how to chain simpler patterns to solve a more complex problem. The key to solving `4Sum` efficiently lies in reducing it to a `2Sum` problem, which is the canonical use case for the **Two Pointers - Converging** pattern.

The signals that point to this pattern are:
1.  **Target Sum:** The problem asks for a combination of elements that sum up to a specific target value.
2.  **Sorted Array:** The pattern requires a sorted array to work. While the input isn't sorted, sorting it is a valid and necessary first step that enables the linear-time two-pointer scan.
3.  **High-Complexity Brute Force:** A brute-force solution with many nested loops (O(Nâ´)) is too slow, signaling the need for a more optimized approach like reducing the problem's dimensionality.

By fixing the first two numbers, we create a subproblem: "find two numbers in a sorted subarray that sum to a value." This is the exact scenario where the converging two-pointer technique shines. It systematically explores all possible pairs in the subarray in a single O(N) pass by starting at opposite ends and moving inwards, efficiently closing the search window based on whether the current sum is too small or too large. Mastering this K-Sum to 2-Sum reduction is fundamental to solving many similar problems.

---

---
<a id="remove-nth-node-from-end-of-list"></a>
### **19. Remove Nth Node From End of List**
**Link to Problem:** [https://leetcode.com/problems/remove-nth-node-from-end-of-list/](https://leetcode.com/problems/remove-nth-node-from-end-of-list/)

#### **1. Problem Statement**
Given the `head` of a singly linked list and an integer `n`, the task is to remove the node that is `n` positions away from the end of the list. The function should then return the head of the potentially modified linked list.

#### **2. Brute Force Approach**
**Intuition:**
The most direct way to solve this is to realize that "nth from the end" is equivalent to "(Length - n + 1)th from the beginning". This insight suggests a two-pass algorithm:

1.  **First Pass:** Traverse the entire linked list once to calculate its total length, let's call it `L`.
2.  **Second Pass:** Calculate the position of the node to remove from the start: `target_pos = L - n`. To remove this node, we actually need to stop at the node *before* it. So, we traverse the list again from the `head`, stopping `target_pos - 1` times to reach the predecessor of the node we want to delete.
3.  **Deletion:** Once at the predecessor node, we update its `next` pointer to skip over the target node, effectively removing it from the list.

A small but important edge case is removing the head of the list itself (when `L == n`). Using a "dummy" or "sentinel" node that points to the original head simplifies this logic, as it ensures every node to be deleted has a predecessor.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:
        # A dummy node helps handle the edge case of removing the actual head node.
        dummy = ListNode(0, head)
        
        # --- First Pass: Calculate the length of the list ---
        length = 0
        current = head
        while current:
            length += 1
            current = current.next
            
        # --- Calculate position of the predecessor node ---
        # The node to remove is at position (length - n) from the start (0-indexed).
        # We need to traverse to the node *before* it.
        # So we start at the dummy and traverse (length - n) times.
        prev = dummy
        for _ in range(length - n):
            prev = prev.next
            
        # --- Second Pass: Remove the target node ---
        # prev is now the node just before the one we want to remove.
        # We bypass the target node by linking prev directly to the target's next node.
        if prev and prev.next:
            prev.next = prev.next.next
            
        # The dummy's next pointer points to the potentially new head of the list.
        return dummy.next
```
**Complexity Analysis:**

*   **Time Complexity:** `O(L)`, where `L` is the number of nodes in the list. We traverse the list twice: once to find the length (`L` steps) and once to find the predecessor node (`L-n` steps). This simplifies to `O(2L)`, which is `O(L)`.
*   **Space Complexity:** `O(1)`. We only use a few extra pointers (`dummy`, `current`, `prev`), so the space used is constant.

### **3. Optimized Approach: [Pattern 3: Two Pointers - Fixed Separation (Nth Node from End)]**
**Intuition:**
The brute-force approach requires two passes because we don't know where the end of the list is relative to a given node. The Two Pointers pattern allows us to solve this in a single pass. The key idea is to create a "fixed separation" or "gap" between two pointers.

1.  Initialize two pointers, `slow` and `fast`. For easier deletion, we'll start `slow` at a `dummy` node pointing to the head, and `fast` at the actual `head`.
2.  **Create the Gap:** First, we move the `fast` pointer `n` steps ahead into the list. Now, `fast` is `n` nodes ahead of `slow` (if we count the `dummy` node). This establishes our fixed separation.
3.  **Move in Tandem:** Next, we advance both `slow` and `fast` one step at a time. They move in parallel, maintaining their fixed separation.
4.  **Find the Target:** We continue this until the `fast` pointer reaches the end of the list (i.e., it becomes `None`). Because of the initial `n`-node gap, when `fast` is at the end, `slow` will be positioned exactly at the node *before* the nth node from the end.
5.  **Deletion:** We can now perform the deletion easily: `slow.next = slow.next.next`.

**Example:** `head = [1,2,3,4,5]`, `n = 2`. We want to remove `4`.

*   `dummy -> 1 -> 2 -> 3 -> 4 -> 5`
*   `slow` starts at `dummy`, `fast` starts at `1`.
*   Move `fast` `n=2` steps: `fast` is now at `3`.
*   Move both until `fast` is `None`:
    *   `slow` -> `1`, `fast` -> `4`
    *   `slow` -> `2`, `fast` -> `5`
    *   `slow` -> `3`, `fast` -> `None`
*   `fast` is `None`. `slow` is at `3`. The node to remove is `slow.next` (which is `4`). Perfect.
*   We set `3.next = 5`. The list becomes `[1,2,3,5]`.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:
        # A dummy node handles the edge case of removing the head node.
        dummy = ListNode(0, head)
        slow = dummy
        fast = head  # Start fast at the actual head
        
        # --- Phase 1: Create a fixed separation between slow and fast ---
        # Move the fast pointer n steps ahead.
        # This creates a gap of 'n' nodes between fast and where slow originally pointed (head).
        for _ in range(n):
            fast = fast.next
            
        # --- Phase 2: Move both pointers in tandem until fast reaches the end ---
        # While fast is not None, both pointers advance one step.
        # When 'fast' reaches the end (None), 'slow' will be perfectly
        # positioned just before the node we need to remove.
        while fast:
            slow = slow.next
            fast = fast.next
            
        # --- Phase 3: Remove the target node ---
        # 'slow' is now at the node just before the nth node from the end.
        # We bypass the target by updating the 'next' pointer.
        slow.next = slow.next.next
        
        # The dummy's 'next' still points to the head of the modified list.
        return dummy.next
```
**Complexity Analysis:**

*   **Time Complexity:** `O(L)`. The pointers traverse the list only once. The `fast` pointer goes from `head` to `None`, and `slow` follows for a portion of that journey. This is a single-pass solution.
*   **Space Complexity:** `O(1)`. We only use a constant number of extra variables (`dummy`, `slow`, `fast`).

#### **4. Pattern Connection**
This problem is a canonical example of the **Two Pointers - Fixed Separation** pattern. This pattern is signaled whenever a problem requires you to find a node or element that is a fixed distance `k` away from another element or, as in this case, from the end of a sequence, especially when you can't easily index from the end (like in a singly linked list).

The brute-force method's need for a preliminary pass to find the total length is the key inefficiency this pattern resolves. Instead of calculating the length `L` to determine the target position `L - n`, the pattern cleverly creates a physical gap of `n` nodes between two pointers. By moving these pointers in unison, this "gap" acts as a moving measurement tool. When the leading pointer (`fast`) reaches a known landmark (the end of the list), the trailing pointer's (`slow`) position is determined by that gap, landing it exactly where it needs to be to solve the problem in a single, efficient pass.

---

---
<a id="remove-duplicates-from-sorted-array"></a>
### **26. Remove Duplicates from Sorted Array**
**Link to Problem:** [https://leetcode.com/problems/remove-duplicates-from-sorted-array/](https://leetcode.com/problems/remove-duplicates-from-sorted-array/)

#### **1. Problem Statement**
Given an integer array `nums` that is sorted in non-decreasing order, the task is to remove the duplicate elements *in-place*. This means each unique element should appear only once, and the relative order of the unique elements must be preserved. The function should return `k`, the number of unique elements, and the first `k` elements of the `nums` array should be modified to contain these unique elements.

#### **2. Brute Force Approach**
**Intuition:**
A common first thought for removing duplicates is to use a data structure that inherently stores only unique items, like a `set`. We could iterate through the input array, add all elements to a set to automatically handle duplicates, and then overwrite the original array with the unique elements from the set. However, since the problem requires the order to be preserved and the array is already sorted, a simpler approach is to create a new list, iterate through the input, and only add an element to our new list if it's different from the last element we added. Finally, we'd copy this new list back into the original `nums` array.

This approach is straightforward but violates the problem's core constraint of performing the operation **in-place** with O(1) extra memory.

**Python Code:**
```python
def removeDuplicates_brute_force(nums: list[int]) -> int:
    # This approach is not truly in-place and uses O(N) extra space,
    # making it a good example of what not to do when faced with this constraint.
    if not nums:
        return 0

    # Use a new list to store unique elements while preserving order.
    unique_elements = []
    unique_elements.append(nums[0]) # The first element is always unique to start.

    for i in range(1, len(nums)):
        # If the current element is different from the last one added, it's unique.
        if nums[i] != unique_elements[-1]:
            unique_elements.append(nums[i])

    # Now, copy the unique elements back into the original array.
    # This is required to modify the input array as requested by the problem.
    for i in range(len(unique_elements)):
        nums[i] = unique_elements[i]
        
    return len(unique_elements)

# Example:
# nums = [0, 0, 1, 1, 1, 2, 2]
# unique_elements becomes [0, 1, 2]
# nums is modified to [0, 1, 2, 1, 1, 2, 2]
# The function returns 3. The caller only considers nums[:3].
```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    This is because we iterate through the input array once to build our `unique_elements` list (O(N)) and then iterate through the `unique_elements` list to modify the original `nums` array (O(k), where k â‰¤ N). This results in a total time complexity of O(N).

*   **Space Complexity: O(N)**
    The primary drawback is the `unique_elements` list, which in the worst case (an array with all unique elements) will grow to the same size as the input array `nums`. This violates the O(1) extra space constraint.

### **3. Optimized Approach: Two Pointers - In-place Array Modification**
**Intuition:**
To solve this efficiently and in-place, we can use the **Two Pointer** technique. The key insight is that since the array is sorted, all duplicate elements will be grouped together. We can use this property to overwrite the duplicates with the next unique elements we find.

We'll define two pointers:
1.  **`slow` (or `write_pointer`)**: This pointer keeps track of the position where the next unique element should be placed. It starts at index 1 because the first element `nums[0]` is always considered unique and stays in its place.
2.  **`fast` (or `read_pointer`)**: This pointer scans the array from the second element (`index = 1`) onwards to find new, unique elements.

The algorithm works as follows: The `fast` pointer iterates through the array. At each step, we compare `nums[fast]` with the previous element `nums[fast - 1]`.
*   If `nums[fast]` is **the same** as `nums[fast - 1]`, it's a duplicate. We do nothing but increment the `fast` pointer to move on.
*   If `nums[fast]` is **different** from `nums[fast - 1]`, it's a unique element. We then copy its value to the position indicated by our `slow` pointer (`nums[slow] = nums[fast]`) and then increment the `slow` pointer to prepare for the next unique element.

Let's walk through an example: `nums = [0, 0, 1, 1, 2]`
- **Initial State:** `slow = 1`, `fast = 1`
- **`fast = 1`**: `nums[1]` (0) is equal to `nums[0]` (0). Duplicate. `fast` increments.
  - `nums` is `[0, 0, 1, 1, 2]`, `slow = 1`
- **`fast = 2`**: `nums[2]` (1) is not equal to `nums[1]` (0). Unique!
  - Copy `nums[fast]` to `nums[slow]`: `nums[1] = 1`.
  - Increment `slow` to 2.
  - `nums` is `[0, 1, 1, 1, 2]`, `slow = 2`
- **`fast = 3`**: `nums[3]` (1) is equal to `nums[2]` (1). Duplicate. `fast` increments.
  - `nums` is `[0, 1, 1, 1, 2]`, `slow = 2`
- **`fast = 4`**: `nums[4]` (2) is not equal to `nums[3]` (1). Unique!
  - Copy `nums[fast]` to `nums[slow]`: `nums[2] = 2`.
  - Increment `slow` to 3.
  - `nums` is `[0, 1, 2, 1, 2]`, `slow = 3`

The loop finishes. The final value of `slow` is 3, which is the number of unique elements. The first `slow` elements of `nums` are `[0, 1, 2]`.

**Python Code:**
```python
def removeDuplicates(nums: list[int]) -> int:
    # If the list is empty, there are no unique elements.
    if not nums:
        return 0

    # 'slow' pointer indicates the next position to write a unique element.
    # It starts at 1 because nums[0] is always in its correct place.
    slow = 1

    # 'fast' pointer iterates through the array to find unique elements.
    for fast in range(1, len(nums)):
        # Check if the current element is a new unique element.
        # Since the array is sorted, a new unique element is one that is
        # different from its immediate predecessor.
        if nums[fast] != nums[fast - 1]:
            # If it's unique, we place it at the 'slow' pointer's position.
            nums[slow] = nums[fast]
            
            # Move the slow pointer forward to mark the new end of the unique subarray.
            slow += 1
            
    # 'slow' now holds the count of unique elements, which is the new length.
    return slow

# Example:
# nums = [0, 0, 1, 1, 1, 2, 2, 3, 3, 4]
# removeDuplicates(nums) returns 5
# nums becomes [0, 1, 2, 3, 4, 2, 2, 3, 3, 4] (the content beyond index 4 doesn't matter)
```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    This is because the `fast` pointer traverses the array from the second element to the end exactly once. The `slow` pointer also moves forward but never exceeds the `fast` pointer. This gives us a single pass through the array.

*   **Space Complexity: O(1)**
    The algorithm is performed entirely in-place. We only use two integer variables (`slow` and `fast`) for the pointers, which is constant extra space regardless of the input array's size.

#### **4. Pattern Connection**
This problem is a classic example of the **Two Pointers - In-place Array Modification** pattern. The pattern is signaled by several key problem characteristics:

1.  **Input is a Sorted Array:** The sorted nature is the most critical clue. It ensures that duplicate elements are adjacent, allowing a linear scan to be effective. If the array were unsorted, we couldn't simply compare an element to its predecessor to check for uniqueness.
2.  **In-place Modification Required:** The problem explicitly forbids using extra space proportional to the input size (O(1) space complexity). This immediately pushes you to think about how to overwrite parts of the array itself.
3.  **Partitioning Logic:** The core task involves conceptually partitioning the array into two sections: the beginning part with the processed, unique elements, and the remaining part with elements yet to be considered. The `slow` pointer acts as the boundary for this partition, while the `fast` pointer ventures into the unprocessed section to find the next item to bring into the "clean" partition.

This pattern, often called the "slow and fast runner" technique, is fundamental for problems where you need to process an array and produce a result in the same array, effectively removing or rearranging elements based on some condition.

---

---
<a id="remove-element"></a>
### **27. Remove Element**
**Link to Problem:** [https://leetcode.com/problems/remove-element/](https://leetcode.com/problems/remove-element/)

#### **1. Problem Statement**
Given an integer array `nums` and an integer `val`, the task is to remove all occurrences of `val` from `nums` **in-place**. The function should return `k`, which is the number of elements remaining in the array after removal. The first `k` elements of the `nums` array must contain the final result, and the order of these `k` elements can be different from their original order.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to iterate through the array and whenever we find an element that needs to be removed, we shift all subsequent elements one position to the left to "overwrite" it. This effectively removes the element and shortens the logical size of the array. We need to be careful when managing our loop index and the array's changing size.

**Python Code:**
```python
def removeElement_brute_force(nums: list[int], val: int) -> int:
    # 'i' is our main iterator, and 'n' is the effective size of the array.
    i = 0
    n = len(nums)
    
    while i < n:
        # If we find the element to remove...
        if nums[i] == val:
            # ...we shift every element after it one position to the left.
            # This is an expensive operation.
            for j in range(i + 1, n):
                nums[j - 1] = nums[j]
            
            # Since we've shifted the array, the effective size decreases by one.
            n -= 1
            
            # We do NOT increment 'i' here, because the new element at nums[i]
            # needs to be checked as well. It could also be 'val'.
        else:
            # If the current element is not 'val', it's fine. Move to the next one.
            i += 1
            
    # 'n' now holds the count of elements that are not 'val'.
    return n

```
**Complexity Analysis:**

*   **Time Complexity: O(N^2)**
    This is because in the worst-case scenario (an array where every element is `val`), the outer `while` loop runs up to `N` times. For each of these iterations, the inner `for` loop (the shifting operation) also runs up to `N` times, leading to a quadratic time complexity.

*   **Space Complexity: O(1)**
    The modification is done in-place, and we only use a few variables (`i`, `n`, `j`) for storage, which does not scale with the input size.

#### **3. Optimized Approach: Two Pointers - In-place Array Modification**
**Intuition:**
The brute-force approach is slow due to the costly O(N) shifting operation performed for every element we remove. We can eliminate this shifting by using a **two-pointer** technique.

The core idea is to partition the array into two sections:
1.  The beginning of the array (from `0` to `k-1`) will store the elements we want to keep.
2.  The rest of the array can be considered a "scratchpad" or ignored.

We'll use two pointers:
*   A "slow" pointer, let's call it `k` (or `write_pointer`), which keeps track of the next position to place an element we want to keep. It starts at index 0.
*   A "fast" pointer, let's call it `i` (or `read_pointer`), which iterates through the entire array to inspect every element.

The `fast` pointer (`i`) moves from left to right. When `nums[i]` is an element we want to **keep** (i.e., `nums[i] != val`), we copy its value to the `slow` pointer's position (`nums[k] = nums[i]`) and then advance the slow pointer (`k += 1`). If `nums[i]` is an element we want to **remove** (i.e., `nums[i] == val`), we simply ignore it and only advance the `fast` pointer.

Let's walk through an example: `nums = [0, 1, 2, 2, 3, 0, 4, 2]`, `val = 2`.

1.  Initialize `k = 0`.
2.  `i = 0`, `nums[0] = 0`. Not `val`. Copy `nums[0]` to `nums[k]`. `nums` becomes `[0, ...]`. Increment `k` to 1.
3.  `i = 1`, `nums[1] = 1`. Not `val`. Copy `nums[1]` to `nums[k]`. `nums` becomes `[0, 1, ...]`. Increment `k` to 2.
4.  `i = 2`, `nums[2] = 2`. This is `val`. Ignore it. `k` remains 2.
5.  `i = 3`, `nums[3] = 2`. This is `val`. Ignore it. `k` remains 2.
6.  `i = 4`, `nums[4] = 3`. Not `val`. Copy `nums[4]` to `nums[k]`. `nums` becomes `[0, 1, 3, ...]`. Increment `k` to 3.
7.  `i = 5`, `nums[5] = 0`. Not `val`. Copy `nums[5]` to `nums[k]`. `nums` becomes `[0, 1, 3, 0, ...]`. Increment `k` to 4.
8.  `i = 6`, `nums[6] = 4`. Not `val`. Copy `nums[6]` to `nums[k]`. `nums` becomes `[0, 1, 3, 0, 4, ...]`. Increment `k` to 5.
9.  `i = 7`, `nums[7] = 2`. This is `val`. Ignore it. `k` remains 5.

The loop finishes. We return `k = 5`. The first 5 elements of `nums` are now `[0, 1, 3, 0, 4]`.

**Python Code:**
```python
def removeElement(nums: list[int], val: int) -> int:
    # 'k' is the slow-runner. It marks the boundary of the processed, valid part of the array.
    # Everything to the left of 'k' is an element we want to keep.
    k = 0
    
    # 'i' is the fast-runner. It iterates through the entire array to inspect each element.
    for i in range(len(nums)):
        # If the element at the fast-runner is NOT the one we want to remove...
        if nums[i] != val:
            # ...we copy it to the position of the slow-runner.
            # This overwrites any 'val' elements that were there or simply rewrites
            # the same element if no 'val's have been found yet.
            nums[k] = nums[i]
            
            # We then advance the slow-runner to expand our "valid" subarray.
            k += 1
            
    # At the end, 'k' is the count of elements that are not 'val', 
    # and the first 'k' elements of the array contain the result.
    return k

```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    This is a significant improvement. Both the `k` (slow) and `i` (fast) pointers traverse the array only once from beginning to end. Each element is read once, and a subset of elements are written to once.

*   **Space Complexity: O(1)**
    The algorithm operates directly on the input array (`in-place`) without allocating any additional data structures, so the space complexity is constant.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - In-place Array Modification** pattern. The signals that point to this pattern are:

1.  **In-place Requirement:** The problem explicitly states that the array must be modified *in-place*, which immediately suggests patterns that avoid creating new arrays.
2.  **Partitioning Task:** The core task is to segregate the array elements into two groups: those that are equal to `val` and those that are not. The pattern uses the slow pointer `k` to effectively create a boundary between the "kept" elements and the "discarded" elements.
3.  **Single Pass Efficiency:** The goal is to solve the problem efficiently, ideally in a single pass. The fast/slow pointer setup allows us to process the array and build the result simultaneously in one O(N) pass, avoiding the inefficient O(N^2) shifting of the brute-force method.

In this pattern, the "slow" pointer builds the final result at the start of the array, while the "fast" pointer explores ahead, finding the next valid piece of data to add to the result. This "read-ahead and write-behind" dynamic is the defining characteristic of using two pointers for in-place array restructuring.

---

---
<a id="sort-colors"></a>
### **75. Sort Colors**
**Link to Problem:** [https://leetcode.com/problems/sort-colors/](https://leetcode.com/problems/sort-colors/)

#### **1. Problem Statement**
Given an array `nums` containing integers representing colors (0 for red, 1 for white, and 2 for blue), the task is to sort the array **in-place**. The final arrangement should have all 0s first, followed by all 1s, and then all 2s. You are not allowed to use the library's built-in sort function.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward approach is to count the occurrences of each color and then overwrite the original array with the correct number of 0s, 1s, and 2s. This is a variation of Counting Sort. The process involves two separate passes over the array: one for counting and one for writing.

1.  **First Pass (Counting):** Iterate through the array and count the number of 0s, 1s, and 2s.
2.  **Second Pass (Overwriting):** Use the counts to rebuild the array. Start from the beginning of the array, fill it with the counted number of 0s, then the 1s, and finally the 2s.

**Python Code:**
```python
def sortColors(nums: list[int]) -> None:
    """
    Sorts the array using a two-pass counting sort algorithm.
    """
    # Step 1: Count the occurrences of each color.
    count_0 = 0
    count_1 = 0
    count_2 = 0
    
    for num in nums:
        if num == 0:
            count_0 += 1
        elif num == 1:
            count_1 += 1
        else:
            count_2 += 1
            
    # Step 2: Overwrite the original array based on the counts.
    # We use an index `i` to keep track of our position in `nums`.
    i = 0
    
    # Place all the 0s.
    for _ in range(count_0):
        nums[i] = 0
        i += 1
        
    # Place all the 1s.
    for _ in range(count_1):
        nums[i] = 1
        i += 1
        
    # Place all the 2s.
    for _ in range(count_2):
        nums[i] = 2
        i += 1

```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    This is because we iterate through the array twice. The first loop to count elements takes O(N) time, and the second phase to overwrite the array also takes O(N) time (N for 0s + N for 1s + N for 2s in total). Thus, the total complexity is O(N) + O(N) = O(N).

*   **Space Complexity: O(1)**
    We only use a few extra variables to store the counts, regardless of the size of the input array. This is constant extra space.

### **3. Optimized Approach: [Pattern 4: Two Pointers - In-place Array Modification]**
**Intuition:**
We can improve upon the two-pass approach by sorting the array in a **single pass** using three pointers. This algorithm is famously known as the **Dutch National Flag problem** solution, a classic example of in-place partitioning.

The core idea is to partition the array into three sections:
1.  A section for `0`s at the beginning.
2.  A section for `1`s in the middle.
3.  A section for `2`s at the end.

We'll use three pointers to manage these sections:
*   `low`: Points to the position where the next `0` should go. Everything to the left of `low` is guaranteed to be a `0`.
*   `high`: Points to the position where the next `2` should go. Everything to the right of `high` is guaranteed to be a `2`.
*   `mid`: The current element being considered. It iterates from the beginning to the end of the array.

The algorithm proceeds as follows, maintaining the invariant `low <= mid <= high`:
- If `nums[mid]` is a `0`, it belongs in the `low` section. We swap `nums[low]` with `nums[mid]` and then increment both `low` and `mid`.
- If `nums[mid]` is a `1`, it's in the correct potential position. We don't need to move it, so we just increment `mid`.
- If `nums[mid]` is a `2`, it belongs in the `high` section. We swap `nums[high]` with `nums[mid]` and then decrement `high`. We **do not** increment `mid` because the new element at `nums[mid]` (which came from `nums[high]`) has not been processed yet and needs to be checked.

Let's walk through an example: `nums = [2, 0, 1]`
- **Initial:** `low = 0`, `mid = 0`, `high = 2`. `nums = [2, 0, 1]`
- **`mid` is at index 0:** `nums[0]` is `2`. Swap `nums[mid]` with `nums[high]`.
  - `nums` becomes `[1, 0, 2]`.
  - Decrement `high`. `high` is now `1`. `mid` remains `0`.
- **`mid` is at index 0:** `nums[0]` is `1`. It's a `1`, so just increment `mid`.
  - `mid` is now `1`.
- **`mid` is at index 1:** `nums[1]` is `0`. Swap `nums[mid]` with `nums[low]`.
  - `nums` becomes `[0, 1, 2]`.
  - Increment `low` and `mid`. `low` is now `1`, `mid` is now `2`.
- **Loop condition:** `mid <= high` (i.e., `2 <= 1`) is now false. The loop terminates.
- **Final:** `nums = [0, 1, 2]`. The array is sorted in a single pass.

**Python Code:**
```python
def sortColors(nums: list[int]) -> None:
    """
    Sorts the array in-place using the Dutch National Flag algorithm (three pointers).
    This is a classic example of the Two Pointers - In-place Array Modification pattern.
    """
    # Pointers to define the boundaries of our three sections.
    # `low` is the boundary for the '0' section.
    # `high` is the boundary for the '2' section.
    low, mid, high = 0, 0, len(nums) - 1
    
    # The main loop continues as long as `mid` has not surpassed `high`.
    # The section between mid and high is the "unprocessed" zone.
    while mid <= high:
        # Case 1: The element at `mid` is a 0.
        if nums[mid] == 0:
            # Swap it with the element at the `low` boundary.
            nums[low], nums[mid] = nums[mid], nums[low]
            # Both `low` and `mid` pointers move one step to the right.
            # We increment `low` because we've placed a 0 correctly.
            # We increment `mid` because the element we swapped from `low` is
            # guaranteed to be a 0 or 1, which `mid` can safely pass.
            low += 1
            mid += 1
            
        # Case 2: The element at `mid` is a 1.
        elif nums[mid] == 1:
            # The element is in its correct potential place, so we just move on.
            mid += 1
            
        # Case 3: The element at `mid` is a 2.
        else: # nums[mid] == 2
            # Swap it with the element at the `high` boundary.
            nums[high], nums[mid] = nums[mid], nums[high]
            # The `high` pointer moves one step to the left, shrinking the
            # "unprocessed" zone from the right.
            high -= 1
            # IMPORTANT: We do NOT increment `mid` here. The new element at `mid`
            # came from the `high` position and we haven't processed it yet.
            # It could be a 0, 1, or 2, and needs to be checked in the next iteration.

```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    Although we have three pointers, the `mid` pointer iterates through the array from the beginning to the end. Each element is visited and processed at most a constant number of times. This results in a single-pass algorithm with linear time complexity.

*   **Space Complexity: O(1)**
    The sorting is performed entirely in-place. We only use three integer variables for our pointers, so the space required is constant and does not depend on the input size.

### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - In-place Array Modification** pattern. The signals that point to this pattern are:

1.  **In-place Requirement:** The problem explicitly forbids creating a new array and demands that the input array be modified directly. This is the strongest indicator for this pattern.
2.  **Partitioning Task:** The core of the problem is not just sorting, but **partitioning** the array into distinct, contiguous sections (`0`s, `1`s, `2`s).
3.  **Defined Boundaries:** The values (`0`, `2`) provide clear criteria for what belongs at the absolute start and absolute end of the array. Pointers (`low`, `high`) are perfect tools for managing the boundaries of these growing, sorted partitions.

By using pointers to track the boundaries of the sorted "red" and "blue" sections, we can iterate through the array once with a third pointer (`mid`), swapping elements into their correct partitions as we find them. This avoids the need for a second pass or extra storage, perfectly demonstrating the power and efficiency of the in-place modification pattern.

---

---
<a id="remove-duplicates-from-sorted-array-ii"></a>
### **80. Remove Duplicates from Sorted Array II**
**Link to Problem:** [https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/](https://leetcode.com/problems/remove-duplicates-from-sorted-array-ii/)

#### **1. Problem Statement**
Given a sorted integer array `nums`, the task is to modify the array in-place to remove duplicate elements so that each unique number appears at most twice. You must return `k`, the length of the modified array, while preserving the relative order of the elements. The elements beyond the `k`-th position do not matter.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this without the in-place constraint would be to build a new array. We could iterate through the input array and add elements to a separate `result` array, but only if they don't violate the "at most twice" rule. After iterating through the entire input, we would copy the contents of our `result` array back into the beginning of the original `nums` array.

This approach is simple to conceptualize but fails the O(1) extra memory constraint, which is a key part of the problem. It serves as a good baseline to understand the requirements before optimizing.

**Python Code:**
```python
def removeDuplicates_brute_force(nums: list[int]) -> int:
    # This approach uses O(N) extra space, violating the problem's constraint.
    # It's presented here to illustrate the basic logic before optimizing.
    if len(nums) <= 2:
        return len(nums)

    # Use a new list to store the valid elements.
    result = []
    for num in nums:
        # We can add the number if the result list is not yet full (size < 2),
        # or if the current number is different from the number two positions back.
        # This check elegantly ensures we don't add a third duplicate.
        if len(result) < 2 or num != result[-2]:
            result.append(num)

    # Copy the valid elements from the result list back to the original nums array.
    for i in range(len(result)):
        nums[i] = result[i]

    # The new length is the size of our result list.
    return len(result)

# Complexity Analysis:
#
# Time Complexity: O(N)
# We iterate through the original array once to build the 'result' list (O(N)),
# and then iterate through the 'result' list to copy elements back (O(k), where k <= N).
# This results in a linear time complexity of O(N).
#
# Space Complexity: O(N)
# We create a new 'result' list that can, in the worst case (no duplicates),
# grow to the same size as the input array. This violates the O(1) space constraint.
```

### **3. Optimized Approach: Two Pointers - In-place Array Modification**
**Intuition:**
To satisfy the O(1) space complexity, we must modify the array in-place. This is the perfect scenario for the **In-place Array Modification** two-pointer pattern.

We can think of the array as having two regions: the processed, valid section at the beginning, and the unprocessed section that we are iterating through.
1.  **`write_ptr` (slow pointer):** This pointer (let's call it `k`) marks the end of the valid section. It indicates the next position where a valid number should be placed. It starts at `0`.
2.  **`read_ptr` (fast pointer):** This pointer simply iterates through every element of the array from beginning to end to examine it.

The core idea is to iterate with the `read_ptr` and decide if the element it's pointing to should be kept. If it should, we copy it to the `write_ptr`'s position and advance the `write_ptr`.

How do we decide if a number `num` should be kept?
A number is valid if it's one of the first two elements we're placing, or if it's different from the element *two positions before* the current `write_ptr`. The element at `nums[k-1]` is the one we just placed, and the one at `nums[k-2]` is the one before that. If our current `num` is the same as `nums[k-2]`, it would be the third instance, which is not allowed.

Let's walk through `nums = [1, 1, 1, 2, 2, 3]`:
-   Initialize `k = 0`.
-   `num = 1`: `k` is 0, which is `< 2`. It's a valid element. So, `nums[0] = 1`, and `k` becomes `1`. Array state: `[1, ...]`.
-   `num = 1`: `k` is 1, which is `< 2`. It's a valid element. So, `nums[1] = 1`, and `k` becomes `2`. Array state: `[1, 1, ...]`.
-   `num = 1`: `k` is 2. We check the condition `num > nums[k-2]`. Is `1 > nums[2-2]` (i.e., `1 > nums[0]`)? No, `1` is not greater than `1`. We don't copy it. `k` remains `2`.
-   `num = 2`: `k` is 2. We check `num > nums[k-2]`. Is `2 > nums[0]`? Yes, `2 > 1`. It's a valid element. So, `nums[2] = 2`, and `k` becomes `3`. Array state: `[1, 1, 2, ...]`.
-   `num = 2`: `k` is 3. We check `num > nums[k-2]`. Is `2 > nums[3-2]` (i.e., `2 > nums[1]`)? Yes, `2 > 1`. It's a valid element. So, `nums[3] = 2`, and `k` becomes `4`. Array state: `[1, 1, 2, 2, ...]`.
-   `num = 3`: `k` is 4. We check `num > nums[k-2]`. Is `3 > nums[4-2]` (i.e., `3 > nums[2]`)? Yes, `3 > 2`. It's a valid element. So, `nums[4] = 3`, and `k` becomes `5`. Array state: `[1, 1, 2, 2, 3, ...]`.

The loop finishes. We return `k=5`, and the first 5 elements of `nums` are correctly `[1, 1, 2, 2, 3]`.

**Python Code:**
```python
def removeDuplicates(nums: list[int]) -> int:
    # 'k' will be the "write pointer". It tracks the index of the last valid element + 1.
    # It essentially represents the length of the valid, modified part of the array.
    k = 0
    
    # The 'for' loop implicitly uses a "read pointer" ('num') to scan the array.
    for num in nums:
        # The condition to keep an element is:
        # 1. We are at the beginning of the array (k < 2), so we can accept up to two elements.
        # OR
        # 2. The current number 'num' is different from the number two positions
        #    before the write pointer. nums[k-2] is the first of the potential duplicates.
        #    If num is greater, it means it's a new number, so we should keep it.
        if k < 2 or num > nums[k-2]:
            # This is a valid element, so we place it at the 'k' position.
            nums[k] = num
            # We advance the write pointer to the next empty slot.
            k += 1
            
    # 'k' now holds the length of the modified array.
    return k

# Complexity Analysis:
#
# Time Complexity: O(N)
# We iterate through the array only once. Both the implicit 'read pointer' (from the loop)
# and the explicit 'write pointer' ('k') traverse the array at most N times.
#
# Space Complexity: O(1)
# The modification is done in-place. We only use a single extra variable 'k' for our pointer,
# achieving the required constant space complexity.
```

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - In-place Array Modification** pattern for several key reasons:

1.  **In-Place Requirement:** The problem explicitly demands an `O(1)` space solution, which is the primary trigger for this pattern. We cannot create a new array, so we must overwrite the input array itself.
2.  **Condensing/Filtering an Array:** The core task is to filter out unwanted elements (excess duplicates) and produce a condensed, valid result at the beginning of the same array.
3.  **Sorted Input:** The sorted nature of the array is a crucial enabler. It guarantees that all duplicate elements are grouped together, which simplifies the logic. We only need to compare the current element with the one at `k-2` to know if we have seen too many duplicates.

The pattern manifests as a "slow" pointer (`k`) that maintains the boundary of the valid, processed prefix of the array, and a "fast" pointer (the loop variable `num`) that scans ahead for the next element to be kept. When the fast pointer finds a suitable element, it gets copied to the slow pointer's location. This "read-ahead and write-behind" mechanism is the defining characteristic of this two-pointer technique for in-place modifications.

---

---
<a id="linked-list-cycle"></a>
### **141. Linked List Cycle**
**Link to Problem:** [https://leetcode.com/problems/linked-list-cycle/](https://leetcode.com/problems/linked-list-cycle/)

#### **1. Problem Statement**
Given the `head` of a singly linked list, the task is to determine if the list has a cycle. A cycle exists if some node in the list can be reached again by continuously following the `next` pointer. The function should return `True` if a cycle is present, and `False` otherwise.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to detect a cycle is to remember every node we've already visited. As we traverse the list from the head, we can store each node in a data structure that allows for quick lookups, like a hash set. If we ever encounter a node that is already in our set of visited nodes, we've confirmed a cycle. If we reach the end of the list (`None`) without seeing any duplicates, no cycle exists.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, x):
#         self.val = x
#         self.next = None

def hasCycle_brute_force(head: ListNode) -> bool:
    # Use a hash set to store nodes we have already seen.
    # Sets provide O(1) average time complexity for add and check operations.
    visited_nodes = set()
    
    current_node = head
    # Traverse the list until we reach the end (current_node is None).
    while current_node:
        # If the current node is already in our set, we've found a cycle.
        if current_node in visited_nodes:
            return True
        
        # If not, add the current node to the set and move to the next one.
        visited_nodes.add(current_node)
        current_node = current_node.next
        
    # If the loop completes, it means we reached the end of the list, so no cycle exists.
    return False

```
**Complexity Analysis:**

*   **Time Complexity:** **O(N)**, where N is the number of nodes in the linked list. In the worst-case scenario (no cycle), we must visit every node once. Operations on the hash set (adding and checking for existence) take O(1) time on average.

*   **Space Complexity:** **O(N)**. In the worst-case scenario (no cycle), the `visited_nodes` set will store a reference to every single node in the list.

### **3. Optimized Approach: Two Pointers - Fast & Slow (Cycle Detection)**
**Intuition:**
This classic problem can be solved far more efficiently without using any extra space by applying the **Fast & Slow Pointers** pattern, also known as **Floyd's Tortoise and Hare Algorithm**.

Imagine two runners on a circular track: a fast runner and a slow runner. If they both start at the same point, the fast runner will eventually lap the slow runner. We can apply this exact analogy to a linked list.

1.  We initialize two pointers, `slow` and `fast`, both pointing to the `head` of the list.
2.  We move the `slow` pointer one step at a time (`slow = slow.next`).
3.  We move the `fast` pointer two steps at a time (`fast = fast.next.next`).

If the list has no cycle, the `fast` pointer will inevitably reach the end of the list (`None`) before the `slow` pointer. If the list *does* have a cycle, the `fast` pointer will enter the cycle first, and the `slow` pointer will follow. Once both pointers are inside the cycle, the `fast` pointer will start gaining on the `slow` pointer from behind and is guaranteed to eventually meet it at the same node.

**Example Walkthrough:**
List: `1 -> 2 -> 3 -> 4 -> 2` (Node 4 points back to Node 2)
*   **Start:** `slow` is at 1, `fast` is at 1.
*   **Step 1:** `slow` moves to 2. `fast` moves to 3.
*   **Step 2:** `slow` moves to 3. `fast` moves to 2 (from 3 -> 4 -> 2).
*   **Step 3:** `slow` moves to 4. `fast` moves to 4 (from 2 -> 3 -> 4).
*   **Meet!** `slow` and `fast` are now at the same node (4). We return `True`.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, x):
#         self.val = x
#         self.next = None

def hasCycle(head: ListNode) -> bool:
    # Handle edge cases: an empty list or a list with one node cannot have a cycle.
    if not head or not head.next:
        return False

    # Initialize two pointers at different speeds.
    # The slow pointer moves one step at a time.
    slow = head
    # The fast pointer moves two steps at a time.
    fast = head.next

    # Traverse the list. The loop continues as long as the pointers haven't met.
    while slow != fast:
        # If the fast pointer or its next node is None, it means we've reached
        # the end of the list, so there is no cycle.
        if not fast or not fast.next:
            return False
        
        # --- Core mechanic of the pattern ---
        # Move the pointers at their respective speeds.
        slow = slow.next
        fast = fast.next.next

    # If the loop terminates because slow == fast, a cycle was found.
    return True

```
**Complexity Analysis:**

*   **Time Complexity:** **O(N)**. Although the `fast` pointer moves twice as fast, the algorithm is still linear with respect to the number of nodes. In the worst case, the slow pointer traverses each node in the non-cyclic part and some part of the cycle before meeting the fast pointer.

*   **Space Complexity:** **O(1)**. This is the key advantage of the pattern. We only use two pointers (`slow` and `fast`) for our traversal, consuming constant extra space regardless of the list's size.

#### **4. Pattern Connection**
This problem is the quintessential example of the **Fast & Slow Pointers** pattern. The pattern is signaled when you need to analyze a sequential data structure, like a linked list, to find a structural property without using extra memory.

The key characteristics that make this pattern a perfect fit are:
1.  **A Traversal Problem:** The core task involves moving through a linked list.
2.  **A Need for Constant Space:** The brute-force solution requires O(N) space, which is often a constraint we want to optimize away.
3.  **Detecting a "Lapping" Condition:** A cycle is fundamentally a state where a traversal will eventually "lap" itself and repeat. By using pointers moving at different speeds, we can deterministically detect this lapping behavior. If one pointer ever catches up to the other, a cycle must exist.

Whenever you encounter a linked list problem asking to find the middle, detect a cycle, or find the start of a cycle, your first thought should be the Fast & Slow Pointers pattern. It transforms a space-intensive hashing problem into an elegant, constant-space pointer manipulation solution.

---

---
<a id="two-sum-ii---input-array-is-sorted"></a>
### **167. Two Sum II - Input Array Is Sorted**
**Link to Problem:** [https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/](https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/)

#### **1. Problem Statement**
Given a 1-indexed array of integers `numbers` that is sorted in non-decreasing order, the task is to find two numbers within it that add up to a specific `target` number. The output should be an array containing the 1-based indices of these two numbers. You can assume there is exactly one solution.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to consider every possible pair of numbers in the array. We can use a nested loop structure. The outer loop selects the first number, and the inner loop iterates through the rest of the array to find a second number. For each pair, we check if their sum equals the target. If it does, we have found our answer.

**Python Code:**
```python
class Solution:
    def twoSum(self, numbers: list[int], target: int) -> list[int]:
        n = len(numbers)
        
        # The outer loop iterates through each element to serve as the first number of our pair.
        for i in range(n):
            # The inner loop iterates through the subsequent elements for the second number.
            # We start from i + 1 to avoid using the same element twice and to avoid duplicate pairs.
            for j in range(i + 1, n):
                # Check if the sum of the current pair matches the target.
                if numbers[i] + numbers[j] == target:
                    # The problem asks for 1-based indices, so we add 1 to our 0-based indices.
                    return [i + 1, j + 1]

```
**Complexity Analysis:**

*   **Time Complexity:** `O(nÂ²)`
    This is due to the nested loops. For each element in the array, we iterate through almost all the other elements, leading to a quadratic number of comparisons in the worst case.

*   **Space Complexity:** `O(1)`
    The space required is constant as we only use a few variables for loop counters and do not allocate any additional data structures whose size depends on the input.

#### **3. Optimized Approach: Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)**
**Intuition:**
The brute-force approach completely ignores a critical piece of information: **the input array is sorted**. This is a massive hint to use a more efficient approach. The "Two Pointers - Converging" pattern is tailor-made for this scenario.

The strategy is as follows:
1.  Initialize two pointers: `left` at the beginning of the array (index 0) and `right` at the end of the array (index `len(numbers) - 1`).
2.  Calculate the sum of the values at these two pointers: `current_sum = numbers[left] + numbers[right]`.
3.  Compare `current_sum` with the `target`:
    *   If `current_sum == target`, we've found our pair. We can return their 1-based indices.
    *   If `current_sum < target`, our sum is too small. Since the array is sorted, the only way to increase the sum is to use a larger number. We achieve this by moving the `left` pointer one step to the right (`left += 1`).
    *   If `current_sum > target`, our sum is too large. We need to decrease it by using a smaller number. We do this by moving the `right` pointer one step to the left (`right -= 1`).
4.  We repeat this process, "converging" the pointers towards each other, until they meet or cross. This systematically eliminates possibilities without ever needing to check every pair.

**Example Walkthrough:** `numbers = [2, 7, 11, 15]`, `target = 9`
- **Start:** `left = 0` (value 2), `right = 3` (value 15).
- **Step 1:** `current_sum = 2 + 15 = 17`.
- **Logic:** `17 > 9` (target), so the sum is too big. We must decrease it.
- **Action:** Move the `right` pointer inward: `right` is now `2`.
- **Step 2:** `left = 0` (value 2), `right = 2` (value 11). `current_sum = 2 + 11 = 13`.
- **Logic:** `13 > 9` (target), still too big.
- **Action:** Move `right` pointer inward again: `right` is now `1`.
- **Step 3:** `left = 0` (value 2), `right = 1` (value 7). `current_sum = 2 + 7 = 9`.
- **Logic:** `9 == 9` (target). We found the solution!
- **Result:** Return `[left + 1, right + 1]`, which is `[1, 2]`.

**Python Code:**
```python
class Solution:
    def twoSum(self, numbers: list[int], target: int) -> list[int]:
        # Initialize two pointers, one at the very beginning and one at the very end.
        left, right = 0, len(numbers) - 1

        # Loop until the two pointers cross each other.
        # Since a solution is guaranteed, the loop will always find it before they cross.
        while left < right:
            # Calculate the sum of the values at the current pointer positions.
            current_sum = numbers[left] + numbers[right]

            if current_sum == target:
                # Found the solution! Return the 1-based indices.
                return [left + 1, right + 1]
            elif current_sum < target:
                # The sum is too small. To increase it, we must use a larger number.
                # The only way to do this is to move the left pointer forward.
                left += 1
            else: # current_sum > target
                # The sum is too large. To decrease it, we must use a smaller number.
                # The only way to do this is to move the right pointer backward.
                right -= 1
```
**Complexity Analysis:**

*   **Time Complexity:** `O(n)`
    In the worst-case scenario, the `left` and `right` pointers will collectively scan the entire array once. Each step of the `while` loop moves one of the pointers closer to the other, so we perform at most `n` comparisons. This is a significant improvement over the `O(nÂ²)` brute-force approach.

*   **Space Complexity:** `O(1)`
    Just like the brute-force method, we only use a fixed number of variables (`left`, `right`, `current_sum`). The memory usage is constant and independent of the input array's size.

#### **4. Pattern Connection**
This problem is the canonical example of the **Two Pointers - Converging** pattern. The signals that point directly to this pattern are:

1.  **A Sorted Array:** This is the most crucial prerequisite. The sorted property is what gives us the logic to move the pointers intelligently. If the current sum is too small, we know for a fact that moving the `left` pointer to the right is the only way to increase the sum. If the sum is too large, moving `right` to the left is the only way to decrease it. Without this sorted property, our logic would fail.
2.  **Finding a Pair or a Target Value:** The goal is to find a *pair* of elements that satisfy a specific condition, usually related to their sum (e.g., equals `target`, is less than `target`, etc.).

By placing pointers at the two extremes and moving them inwards based on their sum, we efficiently narrow down the search space. Each comparison allows us to discard either the current `left` element or the current `right` element, along with all the pairs they could have formed with elements we've already passed. This linear-time traversal is the hallmark of the two-pointer technique on sorted arrays.

---

---
<a id="happy-number"></a>
### **202. Happy Number**
**Link to Problem:** [https://leetcode.com/problems/happy-number/](https://leetcode.com/problems/happy-number/)

#### **1. Problem Statement**
Given a positive integer `n`, determine if it is a "happy number." A number is happy if the process of repeatedly replacing it with the sum of the squares of its digits eventually leads to the number 1. If the process enters a repeating cycle that does not include 1, the number is not happy. The function should return `True` if `n` is happy, and `False` otherwise.

#### **2. Brute Force Approach**
**Intuition:**
The core of this problem is detecting whether the sequence of numbers generated ever repeats itself. If we generate a number we've already seen, we have detected a cycle. The most straightforward way to keep track of the numbers we've already encountered is to use a hash set (or any data structure with fast lookups).

The process is as follows:
1. Start with the given number `n`.
2. In a loop, calculate the next number in the sequence (the sum of the squares of the digits).
3. Before calculating the next number, check if the current number is already in our set of seen numbers.
    * If it is, we've found a cycle. Since we haven't reached 1 yet, this must be an unhappy cycle, so we return `False`.
    * If it isn't, we add the current number to our set and continue.
4. The loop terminates if the number becomes 1 (a happy number) or if a cycle is detected.

**Python Code:**
```python
def isHappy(n: int) -> bool:
    # A set to store numbers we have already seen in the sequence.
    # This is crucial for detecting a cycle.
    seen_numbers = set()

    # The loop continues as long as n is not 1.
    # If we encounter a number already in `seen_numbers`, we've found a cycle.
    while n != 1 and n not in seen_numbers:
        # Add the current number to our record before calculating the next one.
        seen_numbers.add(n)
        
        # Calculate the sum of the squares of the digits.
        sum_of_squares = 0
        current_n = n
        while current_n > 0:
            digit = current_n % 10
            sum_of_squares += digit * digit
            current_n //= 10
        
        # Update n to be the next number in the sequence.
        n = sum_of_squares
        
    # After the loop, if n is 1, it's a happy number.
    # If the loop exited because n was found in `seen_numbers`, n will not be 1.
    return n == 1

```
**Complexity Analysis:**

*   **Time Complexity: O(log n)**
    The time complexity is not dependent on the number of integers we have to check, but rather the "length" of the sequence for a given `n`. The number of digits in `n` is approximately `log10(n)`. Calculating the sum of squares for a number `k` takes `O(log k)` time. The sequence of numbers does not grow infinitely; it is proven that any unhappy number will eventually enter the cycle `4 â†’ 16 â†’ 37 â†’ 58 â†’ 89 â†’ 145 â†’ 42 â†’ 20 â†’ 4`. Thus, the number of steps is bounded by a constant after the initial calculation, leading to an effective `O(log n)` complexity dominated by the initial, larger numbers.

*   **Space Complexity: O(log n)**
    The space complexity is determined by the `seen_numbers` set. In the worst case, we store a number of values proportional to the number of steps it takes to find a cycle. As with time complexity, this is related to the magnitude of the numbers processed, resulting in `O(log n)` space.

---

### **3. Optimized Approach: Two Pointers - Fast & Slow (Cycle Detection)**
**Intuition:**
The hash set approach works, but it requires extra space. We can optimize this by realizing that the problem is fundamentally about **cycle detection in a sequence**. This is a classic use case for Floyd's Cycle-Finding Algorithm, also known as the "Tortoise and the Hare" or **Fast & Slow Pointer** algorithm.

Imagine the sequence of numbers (`n_0`, `n_1`, `n_2`, ...) as a linked list where each number is a node and the "sum of squared digits" function is the `next` pointer.
- `n_1 = get_next(n_0)`
- `n_2 = get_next(n_1)`

We can use two pointers to traverse this conceptual list:
1.  A `slow` pointer that moves one step at a time (`slow = get_next(slow)`).
2.  A `fast` pointer that moves two steps at a time (`fast = get_next(get_next(fast))`).

If there is no cycle, the `fast` pointer will reach the end (in our case, the number 1) first. If there *is* a cycle, the `fast` pointer will eventually enter the cycle and lap the `slow` pointer, meaning they will point to the same number at some point.

**Example Walkthrough (n = 19):**
- **Helper function:** `get_next(num)` calculates the sum of the squares of digits. `get_next(19) = 1Â² + 9Â² = 82`.
- **Initial State:** `slow = 19`, `fast = 19`.
- **Iteration 1:**
    - `slow` moves 1 step: `slow = get_next(19) = 82`.
    - `fast` moves 2 steps: `fast = get_next(get_next(19)) = get_next(82) = 68`.
    - `slow` (82) != `fast` (68).
- **Iteration 2:**
    - `slow` moves 1 step: `slow = get_next(82) = 68`.
    - `fast` moves 2 steps: `fast = get_next(get_next(68)) = get_next(100) = 1`.
    - `slow` (68) != `fast` (1).
- **Termination:** The `fast` pointer has reached 1. We exit the loop and check if `fast == 1`. It is, so we return `True`.

**Python Code:**
```python
def isHappy(n: int) -> bool:
    
    def get_next(number: int) -> int:
        """Helper function to compute the sum of the squares of the digits."""
        total_sum = 0
        while number > 0:
            digit = number % 10
            total_sum += digit * digit
            number //= 10
        return total_sum

    # Initialize two pointers starting from the original number.
    slow_runner = n
    fast_runner = get_next(n)

    # The loop continues as long as the fast runner hasn't reached 1
    # and the two pointers haven't met.
    while fast_runner != 1 and slow_runner != fast_runner:
        # Slow pointer moves one step.
        slow_runner = get_next(slow_runner)
        # Fast pointer moves two steps.
        fast_runner = get_next(get_next(fast_runner))

    # If the loop terminated, one of two conditions was met:
    # 1. fast_runner == 1: We found the happy number termination.
    # 2. slow_runner == fast_runner: A cycle was detected. Since fast_runner is not 1,
    #    it must be an unhappy cycle.
    return fast_runner == 1

```
**Complexity Analysis:**

*   **Time Complexity: O(log n)**
    The time complexity remains the same as the hash set approach. The number of steps until the pointers meet or reach 1 is bounded, and each step's calculation takes `O(log k)` time for a number `k`.

*   **Space Complexity: O(1)**
    This is the key improvement. We are no longer storing a history of seen numbers. We only use a fixed number of variables (`slow_runner`, `fast_runner`) to solve the problem, regardless of the input size. This gives us constant space complexity.

---

#### **4. Pattern Connection**
This problem is a quintessential example of the **Fast & Slow Pointer** pattern for cycle detection, even though it doesn't involve an explicit linked list data structure.

The signal to use this pattern arises when you have a sequence generated by repeatedly applying a function (`n -> f(n) -> f(f(n)) -> ...`), and you need to determine if this sequence ever enters a repeating cycle.

The key characteristics of the "Happy Number" problem that point to this pattern are:
1.  **A Sequence of States:** The problem is defined by a sequence of numbers, where each number is derived from the previous one.
2.  **Two Possible Endings:** The sequence either terminates at a specific value (1) or falls into a loop.
3.  **The Need for Cycle Detection:** The core task is to distinguish between these two outcomes, which is precisely what cycle detection algorithms are for.

By conceptualizing the numbers as nodes and the transformation function as the `next` pointer, the problem becomes identical to "Linked List Cycle II". The Fast & Slow Pointer approach provides a highly efficient, `O(1)` space solution, making it the optimal way to solve problems with this underlying structure.

---

---
<a id="3sum-smaller"></a>
### **259. 3Sum Smaller**
**Link to Problem:** [https://leetcode.com/problems/3sum-smaller/](https://leetcode.com/problems/3sum-smaller/)

#### **1. Problem Statement**
Given an array of integers `nums` and a `target` value, the task is to find the number of unique index triplets `(i, j, k)` where `0 <= i < j < k < n` such that the sum of the corresponding elements `nums[i] + nums[j] + nums[k]` is strictly less than the `target`.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to simply check every possible unique triplet in the array. We can achieve this by using three nested loops. The outer loop will pick the first element `nums[i]`, the second loop will pick `nums[j]` from the elements after `i`, and the innermost loop will pick `nums[k]` from the elements after `j`. For each triplet, we calculate the sum and, if it's less than the `target`, we increment a counter.

**Python Code:**
```python
def threeSumSmaller_brute_force(nums: list[int], target: int) -> int:
    n = len(nums)
    if n < 3:
        return 0
    
    count = 0
    
    # First loop to fix the first element of the triplet
    for i in range(n - 2):
        # Second loop to fix the second element
        for j in range(i + 1, n - 1):
            # Third loop to find the third element
            for k in range(j + 1, n):
                # Check if the sum is smaller than the target
                if nums[i] + nums[j] + nums[k] < target:
                    count += 1
                    
    return count

```
**Complexity Analysis:**

*   **Time Complexity: O(nÂ³)**
    This is due to the three nested loops. In the worst case, each loop runs proportional to the size of the array `n`, leading to a cubic time complexity.

*   **Space Complexity: O(1)**
    We only use a few variables to store the count and loop indices, so the extra space required is constant.

### **3. Optimized Approach: Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)**
**Intuition:**
The `O(nÂ³)` brute force solution is too slow for larger inputs. We can significantly improve performance by first sorting the array. Sorting allows us to use the **Two Pointers - Converging** pattern to efficiently find pairs that satisfy a condition.

The strategy is to iterate through the array with a single `for` loop, fixing one element `nums[i]` at a time. For each `nums[i]`, our goal is to find the number of pairs `(nums[j], nums[k])` in the rest of the array (where `j > i` and `k > j`) such that `nums[j] + nums[k] < target - nums[i]`. This transforms the problem into a "2Sum Smaller" subproblem.

We solve this subproblem using two pointers, `left` and `right`, initialized at the start (`i + 1`) and end of the remaining portion of the array.

Let's walk through an example: `nums = [-2, 0, 1, 3]`, `target = 2`.
1.  **Sort `nums`:** The array is already sorted.
2.  **Outer loop `i = 0` (`nums[i] = -2`):**
    *   We need to find pairs in `[0, 1, 3]` that sum to less than `target - nums[i] = 2 - (-2) = 4`.
    *   Initialize `left = 1`, `right = 3`. The pointers are on `0` and `3`.
    *   `sum = nums[left] + nums[right] = 0 + 3 = 3`.
    *   Since `3 < 4`, we have found valid pairs. The key insight is that if `(nums[left], nums[right])` works, then because the array is sorted, any element between `left` and `right` used as the second element will also work with `nums[left]`.
    *   The pairs are `(0, 3)` and `(0, 1)`. The number of such pairs is `right - left = 3 - 1 = 2`.
    *   We add `2` to our total count and move `left` forward to find new pairs: `left++`.
    *   `left` is now `2`, `right` is `3`. Pointers are on `1` and `3`.
    *   `sum = nums[left] + nums[right] = 1 + 3 = 4`.
    *   `4` is not less than `4`, so the sum is too large. To decrease the sum, we move `right` inward: `right--`.
    *   Now `left = 2`, `right = 2`. The loop terminates as `left` is no longer less than `right`.
3.  **Outer loop `i = 1` (`nums[i] = 0`):**
    *   We need pairs in `[1, 3]` that sum to less than `target - nums[i] = 2 - 0 = 2`.
    *   Initialize `left = 2`, `right = 3`. Pointers are on `1` and `3`.
    *   `sum = 1 + 3 = 4`. `4` is not less than `2`. Sum is too large, so `right--`.
    *   `left` and `right` pointers meet. The loop terminates.
4.  The outer loop finishes. The final count is `2`.

**Python Code:**
```python
def threeSumSmaller(nums: list[int], target: int) -> int:
    # Sorting is the essential first step for the two-pointer pattern.
    nums.sort()
    n = len(nums)
    count = 0
    
    # Iterate through the array, fixing the first element `nums[i]`.
    # We only need to go up to n-2 since we need at least two other elements.
    for i in range(n - 2):
        # Set up two pointers for the rest of the array.
        left, right = i + 1, n - 1
        
        # Use the converging two-pointer technique on the sub-array.
        while left < right:
            current_sum = nums[i] + nums[left] + nums[right]
            
            if current_sum < target:
                # If the sum with the 'right' element is smaller than the target,
                # then any element between 'left' and 'right' will also work
                # as the third element, because the array is sorted.
                # The number of such valid triplets is (right - left).
                count += (right - left)
                
                # To find more potential solutions, we need a larger sum.
                # The only way to increase the sum is to move the 'left' pointer forward.
                left += 1
            else:
                # The sum is too large or equal to the target.
                # To make the sum smaller, we must move the 'right' pointer inward.
                right -= 1
                
    return count

```
**Complexity Analysis:**

*   **Time Complexity: O(nÂ²)**
    The initial sort takes `O(n log n)`. The main logic consists of a `for` loop that runs `n` times, and inside it, a `while` loop with two pointers. For each `i`, the `left` and `right` pointers traverse the sub-array at most once, making the inner part `O(n)`. This results in a total time complexity of `O(n log n + nÂ²)`, which simplifies to `O(nÂ²)`.

*   **Space Complexity: O(log n) to O(n)**
    The space complexity is dominated by the sorting algorithm. In Python, Timsort uses space that can range from `O(log n)` to `O(n)` depending on the data. If we were to implement a sort like Heapsort, it could be `O(1)`.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging (Sorted Array Target Sum)** pattern, extended from a 2Sum to a 3Sum context. The signals that point directly to this pattern are:

1.  **A Sorted Array is Key:** The problem doesn't require a sorted array, but sorting it is the crucial step that unlocks an efficient solution. The ability to logically move pointers inward or outward relies entirely on the sorted property.
2.  **Finding Combinations (Triplets) with a Sum Condition:** The core task is to find triplets that satisfy a condition based on their sum (`< target`). This is a classic setup for target-sum problems.
3.  **Reducing the Problem Space:** By fixing one element with an outer loop, we effectively reduce a `k`-sum problem to a `(k-1)`-sum problem on a subarray. Here, 3Sum Smaller becomes a series of 2Sum Smaller subproblems.
4.  **Converging Pointers for Efficient Search:** For each subproblem, the two pointers starting at opposite ends (`left` and `right`) efficiently scan all possible pairs. If the sum is too small, we move `left` forward; if too large, we move `right` backward. This systematic, linear scan avoids the nested loop of the brute-force approach, dramatically improving performance.

The "smaller than" variation adds a unique twist: when `nums[i] + nums[left] + nums[right] < target`, we don't just count one triplet. We leverage the sorted order to instantly count `right - left` valid triplets, which is the core optimization for this specific problem type.

---

---
<a id="move-zeroes"></a>
### **283. Move Zeroes**
**Link to Problem:** [https://leetcode.com/problems/move-zeroes/](https://leetcode.com/problems/move-zeroes/)

#### **1. Problem Statement**
Given an integer array `nums`, the task is to move all the zeros to the end of it while maintaining the relative order of the non-zero elements. This operation must be performed **in-place**, meaning you cannot create a new copy of the array.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward idea that respects the in-place constraint is to iterate through the array. Whenever we encounter a zero, we can then start a second search from that point forward to find the next non-zero element. Once found, we swap the zero with that non-zero element. This process repeats until we've moved every zero past every non-zero number. This is conceptually similar to a bubble sort, where zeros are "bubbled" to the end.

**Python Code:**
```python
def moveZeroes_brute_force(nums: list[int]) -> None:
    """
    Modifies nums in-place using a brute-force nested loop approach.
    """
    n = len(nums)
    # The outer loop iterates through each element of the array.
    for i in range(n):
        # If we find a zero, we need to find a non-zero element to swap it with.
        if nums[i] == 0:
            # The inner loop searches for the *next* non-zero element.
            for j in range(i + 1, n):
                if nums[j] != 0:
                    # Perform the swap.
                    nums[i], nums[j] = nums[j], nums[i]
                    # Once a swap is made for the zero at index i, we can break
                    # the inner loop and continue the search for the next zero.
                    break

# Example usage:
# nums = [0, 1, 0, 3, 12]
# moveZeroes_brute_force(nums)
# print(nums) -> [1, 3, 12, 0, 0]
```

**Complexity Analysis:**

*   **Time Complexity: O(nÂ²)**
    This is due to the nested loops. In the worst-case scenario (e.g., an array like `[0, 0, 0, 1]`), for each zero found by the outer loop, the inner loop may have to scan a significant portion of the remaining array.

*   **Space Complexity: O(1)**
    The solution is performed in-place. We only use a few variables for indices and swapping, which does not depend on the size of the input array.

#### **3. Optimized Approach: Two Pointers - In-place Array Modification**
**Intuition:**
The brute-force approach is slow because it repeatedly scans parts of the array. We can optimize this by realizing the problem can be rephrased: "bring all non-zero elements to the front of the array." The zeros will naturally be left behind. This is a classic partitioning problem, perfect for the Two Pointers pattern.

We'll use two pointers, let's call them `write_ptr` and `read_ptr`, both starting at the beginning of the array.
1.  `read_ptr`: Its job is to scan the array from left to right, one element at a time.
2.  `write_ptr`: Its job is to keep track of the position where the *next non-zero element* should be placed.

The algorithm works as follows:
*   The `read_ptr` moves forward unconditionally.
*   If `read_ptr` encounters a non-zero element, it means we've found an element that belongs in the "non-zero" section of the array. We copy this element's value to the location of `write_ptr`.
*   After copying, we advance `write_ptr` by one, effectively expanding the "non-zero" section.
*   If `read_ptr` encounters a zero, we simply ignore it and move on. The `write_ptr` stays put, waiting for the next non-zero element to overwrite its position.

**Example Walkthrough:** `nums = [0, 1, 0, 3, 12]`

| `read_ptr` | `nums[read_ptr]` | Action | `write_ptr` | `nums` State |
| :--- | :--- | :--- | :--- | :--- |
| 0 | 0 | Zero found. Do nothing. | 0 | `[0, 1, 0, 3, 12]` |
| 1 | 1 | Non-zero. `nums[write_ptr]` = `nums[read_ptr]`. Increment `write_ptr`. | 1 | `[1, 1, 0, 3, 12]` |
| 2 | 0 | Zero found. Do nothing. | 1 | `[1, 1, 0, 3, 12]` |
| 3 | 3 | Non-zero. `nums[write_ptr]` = `nums[read_ptr]`. Increment `write_ptr`. | 2 | `[1, 3, 0, 3, 12]` |
| 4 | 12 | Non-zero. `nums[write_ptr]` = `nums[read_ptr]`. Increment `write_ptr`. | 3 | `[1, 3, 12, 3, 12]` |

After the first pass, `nums` is `[1, 3, 12, 3, 12]` and `write_ptr` is at index 3. This means all non-zero elements are now correctly ordered in `nums[0...2]`. The final step is to fill the rest of the array (from `write_ptr` to the end) with zeros.

**Python Code:**
```python
def moveZeroes(nums: list[int]) -> None:
    """
    Modifies nums in-place using the two-pointer pattern.
    """
    # write_ptr keeps track of the position to place the next non-zero element.
    write_ptr = 0
    
    # The read_ptr iterates through the entire array.
    for read_ptr in range(len(nums)):
        # If we find a non-zero element with the read_ptr...
        if nums[read_ptr] != 0:
            # ...we place it at the write_ptr's position.
            # This operation is harmless if read_ptr and write_ptr are the same.
            nums[write_ptr] = nums[read_ptr]
            
            # The "non-zero" section has grown, so we advance the write_ptr.
            write_ptr += 1
            
    # After the first pass, all non-zero elements are at the front.
    # The section from write_ptr to the end must be filled with zeros.
    for i in range(write_ptr, len(nums)):
        nums[i] = 0

# Example usage:
# nums = [0, 1, 0, 3, 12]
# moveZeroes(nums)
# print(nums) -> [1, 3, 12, 0, 0]
```

**Complexity Analysis:**

*   **Time Complexity: O(n)**
    This is a significant improvement. We iterate through the array with `read_ptr` once to move the non-zero elements, and then we iterate through the remaining portion with another loop to fill in the zeros. Each element is visited a constant number of times, resulting in a linear time complexity.

*   **Space Complexity: O(1)**
    The solution is performed in-place. We only use two integer pointers, so the space used is constant and does not scale with the input size.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - In-place Array Modification** pattern. The key signals that point to this pattern are:

1.  **In-Place Requirement:** The problem explicitly forbids creating a new array. This is the strongest hint to consider an in-place algorithm, and two pointers are a primary tool for such tasks.
2.  **Array Partitioning:** The core task is to partition the array into two distinct, contiguous groups: non-zero elements at the beginning and zero elements at the end.
3.  **Conditional Logic:** The decision to move an element is based on a simple condition (`is the element zero?`).

The pattern works by using one pointer (`write_ptr`) to maintain the boundary of the "processed" or "correct" section of the array (in this case, the non-zeros). The second pointer (`read_ptr`) ventures into the "unprocessed" section to find elements that satisfy the condition and should be moved into the correct section. This "slow and fast pointer" dynamic is a common and powerful technique for in-place array manipulation. Whenever you need to segregate elements of an array into two groups in-place, this pattern should be one of the first you consider.

---

---
<a id="find-the-duplicate-number"></a>
### **287. Find the Duplicate Number**
**Link to Problem:** [https://leetcode.com/problems/find-the-duplicate-number/](https://leetcode.com/problems/find-the-duplicate-number/)

#### **1. Problem Statement**
You are given an array of `n + 1` integers, `nums`, where each integer is in the range `[1, n]` inclusive. Since there are `n+1` numbers but only `n` possible values, at least one number must be repeated. Your task is to find this single duplicate number, with the critical constraints that you **cannot modify the input array** and must use only **constant, O(1) extra space**.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to find a duplicate is to compare every number with every other number in the array. We can use a nested loop: the outer loop picks an element, and the inner loop iterates through the rest of the array to see if a matching element exists. If a match is found, we have found our duplicate.

**Python Code:**
```python
def findDuplicate_brute(nums: list[int]) -> int:
    """
    Finds the duplicate number by comparing every element with every other element.
    """
    n = len(nums)
    # The outer loop picks an element one by one.
    for i in range(n):
        # The inner loop checks if this element appears again later in the array.
        for j in range(i + 1, n):
            # If we find two elements at different indices with the same value,
            # we have found the duplicate.
            if nums[i] == nums[j]:
                return nums[i]
    return -1 # Should not be reached given the problem constraints
```
**Complexity Analysis:**

*   **Time Complexity:** `O(n^2)`. For each element, we scan the remainder of the array. This results in a nested loop structure, leading to quadratic time complexity.
*   **Space Complexity:** `O(1)`. We only use a few variables for loop indices, requiring no extra space that scales with the input size.

#### **3. Optimized Approach: Two Pointers - Fast & Slow (Cycle Detection)**
Before diving into the optimal solution, it's worth noting that common `O(n)` solutions are invalid here. Using a hash set would take `O(n)` space, and sorting the array would take `O(n log n)` time and violate the "no modification" rule. This is why a more creative approach is needed.

**Intuition:**
The key insight is to re-frame the problem from finding a duplicate in an array to detecting a cycle in a linked list. We can imagine the array `nums` as a special kind of linked list where the value at each index `i` is a pointer to the next index, `nums[i]`.

For example, if `nums = [1, 3, 4, 2, 2]`:
*   A "node" at index `0` points to index `1` (since `nums[0] = 1`).
*   A "node" at index `1` points to index `3` (since `nums[1] = 3`).
*   A "node" at index `3` points to index `2` (since `nums[3] = 2`).
*   Both index `3` and index `4` point to index `2` (since `nums[3] = 2` and `nums[4] = 2`).

This structure guarantees a cycle. Since all numbers are between `1` and `n`, and indices are from `0` to `n`, every "pointer" `nums[i]` leads to a valid subsequent index. Because there are `n+1` "pointers" (the numbers) but only `n` distinct indices to point to (indices `1` to `n`), at least two pointers must point to the same index. This convergence creates a path that inevitably leads into a cycle. The duplicate number is the entry point of this cycle.

We can find this entry point using **Floyd's Tortoise and Hare (Cycle Detection) algorithm**:

1.  **Phase 1: Find the intersection point inside the cycle.**
    *   Initialize two pointers, `slow` and `fast`, at the start of the sequence (`nums[0]`).
    *   Move `slow` one step at a time (`slow = nums[slow]`).
    *   Move `fast` two steps at a time (`fast = nums[nums[fast]]`).
    *   Eventually, they will meet somewhere inside the cycle.

2.  **Phase 2: Find the entrance of the cycle.**
    *   Once they meet, reset one pointer (e.g., `slow`) back to the beginning (`nums[0]`).
    *   Keep the other pointer (`fast`) at the intersection point.
    *   Move both pointers one step at a time.
    *   The point where they meet again is the entrance to the cycle, which is our duplicate number.

**Python Code:**
```python
def findDuplicate(nums: list[int]) -> int:
    """
    Finds the duplicate number using Floyd's Tortoise and Hare algorithm.
    This treats the array as a functional graph and finds the cycle entrance.
    """
    # Phase 1: Find the intersection point of the two pointers.
    # The 'slow' pointer moves one step at a time.
    # The 'fast' pointer moves two steps at a time.
    slow, fast = 0, 0
    while True:
        slow = nums[slow]
        fast = nums[nums[fast]]
        # The pointers will eventually meet inside the cycle.
        if slow == fast:
            break
            
    # Phase 2: Find the entrance to the cycle.
    # Reset one pointer to the start of the array.
    slow2 = 0
    while True:
        # Move both pointers one step at a time.
        slow = nums[slow]
        slow2 = nums[slow2]
        # The point where they meet is the start of the cycle,
        # which corresponds to the duplicate number.
        if slow == slow2:
            return slow
```
**Complexity Analysis:**

*   **Time Complexity:** `O(n)`. In the first phase, the slow pointer travels at most `n` steps before entering the cycle. The fast pointer catches up within the cycle in at most `n` more steps. The second phase also takes at most `n` steps. The total time is linear.
*   **Space Complexity:** `O(1)`. We only use a few variables (`slow`, `fast`, `slow2`) to store pointers, fully satisfying the problem's constraints.

#### **4. Pattern Connection**
This problem is a quintessential, albeit disguised, example of the **Fast & Slow Pointers (Cycle Detection)** pattern. While it's presented as an array problem, its constraints create an underlying structure that is equivalent to a linked list with a cycle.

The key characteristics that signal this pattern are:
1.  **An Implicit Sequence:** The problem can be modeled as a sequence where each element points to another (`index -> value -> next_index`). This forms a functional graph.
2.  **Guaranteed Cycle:** The problem's constraints (`n+1` numbers in the range `[1,n]`) ensure that this sequence isn't just a simple path; it must contain a cycle. The "duplicate number" is the cause and the entry point of this cycle.
3.  **The Goal is the Cycle's Start:** The objective is not just to detect a cycle but to find its starting point, which is precisely what the second phase of Floyd's algorithm is designed to do.

Recognizing that an array problem can be transformed into a graph/linked list traversal is a powerful problem-solving skill. Whenever you encounter problems involving sequences, permutations, or arrays where values can be interpreted as indices, and you need to find duplicates or loops under strict memory constraints, the Fast & Slow Pointer pattern should be one of the first things you consider.

---

---
<a id="intersection-of-two-arrays"></a>
### **349. Intersection of Two Arrays**
**Link to Problem:** [https://leetcode.com/problems/intersection-of-two-arrays/](https://leetcode.com/problems/intersection-of-two-arrays/)

#### **1. Problem Statement**
Given two integer arrays, `nums1` and `nums2`, the task is to return an array containing their intersection. The key constraints are that each element in the result must be **unique**, and the order of the elements in the output does not matter.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to check every element of the first array against every element of the second array. We can iterate through `nums1`, and for each element, we scan the entirety of `nums2` to see if a match exists. To handle the "unique" requirement, we can use a hash set to store the common elements we find, which automatically prevents duplicates.

**Python Code:**
```python
def intersection_brute_force(nums1, nums2):
    # Use a set to store the intersection to automatically handle uniqueness.
    intersection_set = set()
    
    # Iterate through each number in the first array.
    for n1 in nums1:
        # For each number, iterate through the entire second array to look for a match.
        for n2 in nums2:
            if n1 == n2:
                # If a match is found, add it to our set.
                # If the element is already in the set, this operation does nothing.
                intersection_set.add(n1)
                # We can break here since we only care if it exists, not how many times.
                break
                
    # The problem asks for an array (list in Python) as output.
    return list(intersection_set)

# Complexity Analysis:
#
# Time Complexity: O(n * m)
# Where 'n' is the length of nums1 and 'm' is the length of nums2. This is because
# for every element in nums1, we potentially iterate through all elements of nums2,
# leading to a nested loop structure.
#
# Space Complexity: O(k)
# Where 'k' is the number of unique elements in the intersection. In the worst case,
# if the smaller array is a subset of the larger one, the space complexity would be
# O(min(n, m)) to store the result set.
```

#### **3. Optimized Approach: Two Pointers on Sorted Arrays**
*(Note: While categorized under "Converging Pointers", this problem uses a variation where two pointers traverse two separate arrays in the same direction, rather than converging from opposite ends of a single array. The core principle of using pointers to avoid redundant checks remains the same.)*

**Intuition:**
The brute-force approach is slow because for each element in `nums1`, we repeatedly search `nums2` from the beginning. We can do much better if the arrays are ordered.

The key insight is to **sort both arrays first**. Once sorted, we can use two pointers, one for each array, and traverse them in a single, linear pass. This is the essence of the Two Pointers pattern for this problem.

Let's walk through an example:
`nums1 = [4, 9, 5]`, `nums2 = [9, 4, 9, 8, 4]`

1.  **Sort:**
    *   `nums1` becomes `[4, 5, 9]`
    *   `nums2` becomes `[4, 4, 8, 9, 9]`

2.  **Initialize Pointers:**
    *   `p1` points to `nums1[0]` (value 4).
    *   `p2` points to `nums2[0]` (value 4).

3.  **Compare and Move:**
    *   **Iteration 1:** `nums1[p1]` (4) == `nums2[p2]` (4). We found an intersection! Add 4 to our result set. Advance **both** pointers.
        *   `p1` is now at index 1 (value 5). `p2` is at index 1 (value 4).
    *   **Iteration 2:** `nums1[p1]` (5) > `nums2[p2]` (4). The value in `nums2` is too small. To find a potential match for 5, we must look at a larger number in `nums2`. Advance `p2`.
        *   `p1` is at index 1 (value 5). `p2` is at index 2 (value 8).
    *   **Iteration 3:** `nums1[p1]` (5) < `nums2[p2]` (8). Now the value in `nums1` is too small. To find a potential match for 8, we must look at a larger number in `nums1`. Advance `p1`.
        *   `p1` is at index 2 (value 9). `p2` is at index 2 (value 8).
    *   **Iteration 4:** `nums1[p1]` (9) > `nums2[p2]` (8). The value in `nums2` is too small. Advance `p2`.
        *   `p1` is at index 2 (value 9). `p2` is at index 3 (value 9).
    *   **Iteration 5:** `nums1[p1]` (9) == `nums2[p2]` (9). Another intersection! Add 9 to the result set. Advance **both** pointers.
        *   `p1` is at index 3 (out of bounds).
    *   The `while` loop terminates because `p1` is out of bounds. The final result is `{4, 9}`.

**Python Code:**
```python
def intersection_two_pointers(nums1, nums2):
    # The two-pointer approach requires sorted arrays to work correctly.
    # This is the crucial pre-processing step.
    nums1.sort()
    nums2.sort()
    
    # Initialize pointers at the beginning of each array.
    p1, p2 = 0, 0
    result_set = set()
    
    # The main loop continues as long as both pointers are within their array's bounds.
    while p1 < len(nums1) and p2 < len(nums2):
        # Case 1: The elements are equal, we found an intersection.
        if nums1[p1] == nums2[p2]:
            result_set.add(nums1[p1])
            # Advance both pointers to look for the next potential intersection.
            p1 += 1
            p2 += 1
        # Case 2: The element in nums1 is smaller.
        # We need to advance p1 to find a potentially larger value to match nums2[p2].
        elif nums1[p1] < nums2[p2]:
            p1 += 1
        # Case 3: The element in nums2 is smaller.
        # We need to advance p2 to find a potentially larger value to match nums1[p1].
        else: # nums1[p1] > nums2[p2]
            p2 += 1
            
    return list(result_set)

# Complexity Analysis:
#
# Time Complexity: O(n log n + m log m)
# The dominant operation is sorting. Sorting nums1 takes O(n log n) and nums2 takes
# O(m log m). The subsequent two-pointer scan takes only O(n + m) because each
# pointer moves forward and traverses its array just once. The overall complexity
# is therefore determined by the sorting step.
#
# Space Complexity: O(k) or O(n + m)
# The space for the output set is O(k), where 'k' is the number of intersection elements.
# However, it's important to note that the sorting algorithm itself might require auxiliary space.
# Python's Timsort can use up to O(n) space in the worst case. If we are not allowed to
# modify the input arrays, we would need O(n + m) space to store the sorted copies.
```

#### **4. Pattern Connection**
This problem is a classic example of the **Two Pointers** pattern, specifically the variant applied to two separate arrays. The key signal for this pattern is when you need to find pairs of elements or commonalities between **two sorted sequences**.

The brute-force solution's inefficiency stems from its O(n*m) comparisons. Sorting the arrays is the critical enabler. It imposes an order that allows us to discard parts of the search space intelligently. If `nums1[p1]` is smaller than `nums2[p2]`, we know for a fact that `nums1[p1]` cannot match `nums2[p2]` or any subsequent elements in `nums2` (since `nums2` is sorted). Therefore, we can safely advance `p1` without ever looking back. This single-pass, coordinated movement of two pointers after a sorting pre-step is the hallmark of this pattern, effectively reducing the comparison time complexity from quadratic to linear.

---

---
<a id="is-subsequence"></a>
### **392. Is Subsequence**
**Link to Problem:** [https://leetcode.com/problems/is-subsequence/](https://leetcode.com/problems/is-subsequence/)

#### **1. Problem Statement**
Given two strings, `s` and `t`, the task is to determine if `s` is a subsequence of `t`. A subsequence is formed by deleting zero or more characters from the original string (`t`) without changing the relative order of the remaining characters. For example, "ace" is a subsequence of "abcde", but "aec" is not.

#### **2. Brute Force Approach**
**Intuition:**
A straightforward way to think about this is to iterate through the potential subsequence `s` character by character. For each character in `s`, we need to find its first occurrence in `t` *after* the position of the previous character we found. We can maintain a "search-from" index for `t`. If we ever fail to find a character, we know `s` cannot be a subsequence.

The logic is as follows:
1.  Initialize a pointer, `search_from_index`, to 0. This tracks where in `t` we should start our next search.
2.  Iterate through each character `char_s` in string `s`.
3.  For each `char_s`, search for it within string `t`, but only starting from `search_from_index`.
4.  If `char_s` is found at a position `found_pos`, we update `search_from_index` to `found_pos + 1` for the next iteration. This ensures we only look forward in `t`, preserving the relative order.
5.  If `char_s` is not found in the remainder of `t`, it's impossible to form the subsequence. We can immediately return `False`.
6.  If the loop completes, it means every character in `s` was found in `t` in the correct order, so we return `True`.

**Python Code:**
```python
def isSubsequence_brute_force(s: str, t: str) -> bool:
    # This index tracks where our search should begin in the target string 't'.
    search_from_index = 0

    # Iterate through each character of the potential subsequence 's'.
    for char_s in s:
        # str.find(substring, start_index) is a convenient way to implement this logic.
        # It searches for char_s in t, starting from our current search position.
        found_pos = t.find(char_s, search_from_index)

        # If find() returns -1, the character was not found in the rest of string 't'.
        if found_pos == -1:
            return False  # s cannot be a subsequence.
        
        # If the character was found, we must start the search for the *next*
        # character of 's' *after* the current character's position.
        search_from_index = found_pos + 1
    
    # If we successfully exit the loop, all characters of 's' were found in order.
    return True

```
**Complexity Analysis:**

*   **Time Complexity: O(S * T)**, where `S` is the length of `s` and `T` is the length of `t`. In the worst case (e.g., `s = "aaaa"` and `t = "aaaaa"`), for each of the `S` characters in `s`, the `t.find()` method might scan a large portion of `t`.
*   **Space Complexity: O(1)**. We only use a few variables for storage, regardless of the input string sizes.

---
### **3. Optimized Approach: Two Pointers - Fast & Slow (Cycle Detection)**
**Intuition:**
The brute-force approach is inefficient because it may re-scan parts of string `t` repeatedly. We can optimize this by making a single pass through `t`. The key insight is to use two pointers to track our progress through both strings simultaneously. This is a classic application of the Two Pointers pattern.

Let's call our pointers `s_ptr` (for string `s`) and `t_ptr` (for string `t`). Both start at index 0.

1.  `s_ptr` points to the character in `s` we are currently looking for.
2.  `t_ptr` scans through string `t`.

The algorithm works by advancing `t_ptr` through `t` and only advancing `s_ptr` when we find a match. This ensures we check for characters in the correct relative order.

Let's trace `s = "ace"`, `t = "abcde"`:
*   **Initial:** `s_ptr = 0` (points to 'a'), `t_ptr = 0` (points to 'a').
*   **Step 1:** `s[s_ptr]` ('a') == `t[t_ptr]` ('a'). It's a match! We've found the first character of `s`. We advance **both** pointers to look for the next character.
    *   `s_ptr` becomes 1, `t_ptr` becomes 1.
*   **Step 2:** `s[s_ptr]` ('c') != `t[t_ptr]` ('b'). No match. We haven't found the 'c' we're looking for yet. We advance only `t_ptr` to continue scanning `t`.
    *   `s_ptr` stays 1, `t_ptr` becomes 2.
*   **Step 3:** `s[s_ptr]` ('c') == `t[t_ptr]` ('c'). A match! We advance **both**.
    *   `s_ptr` becomes 2, `t_ptr` becomes 3.
*   **Step 4:** `s[s_ptr]` ('e') != `t[t_ptr]` ('d'). No match. Advance only `t_ptr`.
    *   `s_ptr` stays 2, `t_ptr` becomes 4.
*   **Step 5:** `s[s_ptr]` ('e') == `t[t_ptr]` ('e'). A match! Advance **both**.
    *   `s_ptr` becomes 3, `t_ptr` becomes 5.

The loop terminates when one of the pointers goes out of bounds. The final check is key: if `s_ptr` has successfully reached the end of `s` (i.e., `s_ptr == len(s)`), it means every character of `s` was found in order.

**Python Code:**
```python
def isSubsequence_optimized(s: str, t: str) -> bool:
    # s_ptr tracks our progress in the subsequence 's'.
    # t_ptr tracks our progress in the main string 't'.
    s_ptr, t_ptr = 0, 0

    # We continue as long as both pointers are within the bounds of their respective strings.
    while s_ptr < len(s) and t_ptr < len(t):
        # If the characters match, it means we've found the character s[s_ptr] in 't'.
        # We can now look for the *next* character in 's'.
        if s[s_ptr] == t[t_ptr]:
            s_ptr += 1  # Move the 'slow' pointer only on a match.
        
        # We always advance t_ptr to scan through the main string.
        # This is the 'fast' pointer that moves unconditionally.
        t_ptr += 1
    
    # After the loop, if s_ptr has reached the length of s,
    # it means all characters of s were found in order.
    return s_ptr == len(s)

```
**Complexity Analysis:**

*   **Time Complexity: O(T)**, where `T` is the length of `t`. We traverse the string `t` with `t_ptr` exactly once. The `s_ptr` only moves forward and does not add to the overall complexity. This is a significant improvement over O(S * T).
*   **Space Complexity: O(1)**. We only use two integer variables for the pointers.

---
### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers** pattern. While it's not the "Fast & Slow" variant typically used for cycle detection in a single data structure, it perfectly demonstrates the power of using two pointers to process sequences in lockstep.

The key characteristics that signal the Two Pointers pattern here are:
1.  **Comparison between two sequences:** We need to compare characters from `s` and `t`.
2.  **Order matters:** The problem requires maintaining the relative order of characters, making a simple frequency count (like with a hash map) insufficient.
3.  **Linear scan:** The problem can be solved by iterating through the strings without complex nested structures.

The two pointers, `s_ptr` and `t_ptr`, efficiently manage state: `s_ptr` represents the "requirement" (which character we are looking for), and `t_ptr` represents the "progress" (where we are in our search). One pointer (`t_ptr`) moves steadily forward, while the other (`s_ptr`) moves conditionally. This "different rate of movement" is the core principle that connects this problem to the broader family of Two Pointer algorithms, allowing us to solve it elegantly in a single, linear pass.

---

---
<a id="string-compression"></a>
### **443. String Compression**
**Link to Problem:** [https://leetcode.com/problems/string-compression/](https://leetcode.com/problems/string-compression/)

#### **1. Problem Statement**
Given an array of characters `chars`, compress it in-place. The compression logic is to replace consecutive repeating characters with the character itself followed by its count. If a group's length is 1, the number '1' should not be appended. The function must return the new length of the compressed array.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward idea is to ignore the "in-place" constraint initially to simplify the logic. We can build the compressed result in a separate data structure (like a list of characters or a string builder). After processing the entire input array, we can then copy this new, compressed result back into the beginning of the original `chars` array. This separates the logic of compression from the complexity of in-place modification.

**Python Code:**
```python
def compress_brute_force(chars: list[str]) -> int:
    # This approach is not truly in-place and uses O(N) extra space.
    if not chars:
        return 0

    # Use an auxiliary list to build the compressed result.
    compressed_result = []
    
    i = 0
    n = len(chars)
    while i < n:
        current_char = chars[i]
        count = 0
        
        # Count consecutive occurrences of the current character.
        j = i
        while j < n and chars[j] == current_char:
            count += 1
            j += 1
        
        # Append the character to our result.
        compressed_result.append(current_char)
        
        # If the count is greater than 1, append the digits of the count.
        if count > 1:
            for digit in str(count):
                compressed_result.append(digit)
        
        # Move the main pointer to the start of the next new character.
        i = j
        
    # Now, copy the compressed result back into the original `chars` array.
    for i in range(len(compressed_result)):
        chars[i] = compressed_result[i]
    
    # The new length is the length of our auxiliary list.
    return len(compressed_result)

# Example Usage:
# chars = ["a","a","b","b","c","c","c"]
# new_length = compress_brute_force(chars)
# print(new_length)  # Output: 6
# print(chars[:new_length]) # Output: ['a', '2', 'b', '2', 'c', '3']
```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    The `while` loop iterates through the input `chars` array once to build the `compressed_result`, which takes O(N) time. The final `for` loop to copy the result back into `chars` also takes O(N) in the worst case. Therefore, the total time complexity is O(N) + O(N) = O(N).

*   **Space Complexity: O(N)**
    This is the main drawback. We use an auxiliary list, `compressed_result`, which in the worst-case scenario (e.g., `["a","b","c"]`) could grow to be the same size as the input array. This violates the problem's O(1) space constraint.

### **3. Optimized Approach: Pattern 4: Two Pointers - In-place Array Modification**
**Intuition:**
To solve this in-place, we need a way to overwrite the input array `chars` without losing information we still need to read. This is a perfect scenario for a "read/write" two-pointer approach.

We'll use two pointers:
1.  A `read` pointer (`i` in the code) that scans through the original array.
2.  A `write` pointer (`write_idx`) that marks the position in the array where the next piece of compressed information (a character or a digit) should be written.

The `read` pointer will always be ahead of or at the same position as the `write` pointer. The `read` pointer's job is to find the boundaries of a group of consecutive characters. Once a group is identified (e.g., "a", "a", "a"), the `write` pointer writes the character ('a') and its count ('3') to the front of the array. This process overwrites the old data, which is safe because the `read` pointer has already scanned past it.

Let's walk through `chars = ["a", "a", "b", "b", "c", "c", "c"]`:

1.  Initialize `read = 0` and `write_idx = 0`.
2.  **Group 'a':** The `read` pointer scans forward and finds two 'a's.
    *   Write the character: `chars[write_idx] = 'a'`. `write_idx` is now 1.
    *   Count is 2. Convert '2' to a character and write it: `chars[write_idx] = '2'`. `write_idx` is now 2.
    *   `chars` is now `["a", "2", "b", "b", "c", "c", "c"]`.
3.  **Group 'b':** The `read` pointer is now at the first 'b' (index 2). It scans forward and finds two 'b's.
    *   Write the character: `chars[write_idx] = 'b'`. `write_idx` is now 3.
    *   Count is 2. Write '2': `chars[write_idx] = '2'`. `write_idx` is now 4.
    *   `chars` is now `["a", "2", "b", "2", "c", "c", "c"]`.
4.  **Group 'c':** The `read` pointer is now at the first 'c' (index 4). It scans forward and finds three 'c's.
    *   Write the character: `chars[write_idx] = 'c'`. `write_idx` is now 5.
    *   Count is 3. Write '3': `chars[write_idx] = '3'`. `write_idx` is now 6.
    *   `chars` is now `["a", "2", "b", "2", "c", "3", "c"]`.
5.  The `read` pointer reaches the end. The final value of `write_idx` (6) is the new length.

**Python Code:**
```python
def compress(chars: list[str]) -> int:
    # write_idx is our slow "write" pointer.
    # It tracks the end of the compressed section of the array.
    write_idx = 0
    
    # read_ptr is our fast "read" pointer.
    # It scans the original array.
    read_ptr = 0
    n = len(chars)

    while read_ptr < n:
        current_char = chars[read_ptr]
        count = 0
        
        # Use a separate pointer to find the end of the current group of characters.
        group_end_ptr = read_ptr
        while group_end_ptr < n and chars[group_end_ptr] == current_char:
            count += 1
            group_end_ptr += 1
            
        # --- Perform the In-place Write ---
        # 1. Write the character itself.
        chars[write_idx] = current_char
        write_idx += 1
        
        # 2. If the count is > 1, write the digits of the count.
        if count > 1:
            for digit in str(count):
                chars[write_idx] = digit
                write_idx += 1
        
        # Move the read pointer to the start of the next group.
        read_ptr = group_end_ptr
        
    # The final position of the write_idx is the new length of the array.
    return write_idx

# Example Usage:
# chars = ["a","a","b","b","c","c","c"]
# new_length = compress(chars)
# print(new_length)  # Output: 6
# print(chars[:new_length]) # Output: ['a', '2', 'b', '2', 'c', '3']
```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    Although we have a nested `while` loop, this is a single-pass solution. The outer `read_ptr` and the inner `group_end_ptr` together ensure that each character in the `chars` array is visited exactly once. Therefore, the time complexity is linear.

*   **Space Complexity: O(1)**
    This is the key improvement. We modify the array in-place and only use a few variables (`write_idx`, `read_ptr`, `count`, etc.) for tracking. The space required does not depend on the size of the input array.

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - In-place Array Modification** pattern. The pattern is signaled by two key constraints:
1.  The modification must happen **in-place**.
2.  The result of the modification is guaranteed to be **shorter than or equal to** the original array length.

These constraints create a scenario where you can safely overwrite the beginning of the array while reading from a point further ahead. The "write" pointer (`write_idx`) constructs the new, valid array at the front, while the "read" pointer (`read_ptr`) explores the yet-unprocessed part of the original array. This separation of reading and writing duties is the core concept of the pattern, allowing for efficient O(1) space solutions to problems that would otherwise seem to require extra memory. Problems like "Remove Duplicates from Sorted Array" (LeetCode 26) and "Move Zeroes" (LeetCode 283) follow this exact same structural logic.

---

---
<a id="middle-of-the-linked-list"></a>
### **876. Middle of the Linked List**
**Link to Problem:** [https://leetcode.com/problems/middle-of-the-linked-list/](https://leetcode.com/problems/middle-of-the-linked-list/)

#### **1. Problem Statement**
Given the `head` of a singly linked list, the task is to find and return the middle node of that list. If the list contains an even number of nodes, there will be two middle nodes; in this case, the second middle node should be returned.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to find the middle of something is to first know its total size. The brute-force approach follows this logic directly. We can solve this in two distinct passes over the linked list:
1.  **First Pass (Count):** Traverse the entire list from the `head` to the end, counting the total number of nodes (`N`).
2.  **Second Pass (Find):** Calculate the index of the middle node, which is `N // 2`. Then, start another traversal from the `head` and stop after `N // 2` steps. The node at this position is our answer.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def middleNode(self, head: Optional[ListNode]) -> Optional[ListNode]:
        # --- First Pass: Count the total number of nodes ---
        count = 0
        current = head
        # Traverse the list to its end to get the total count.
        while current:
            count += 1
            current = current.next

        # --- Second Pass: Traverse to the middle node ---
        # Calculate the index of the middle node.
        # For a list of 5 nodes (0,1,2,3,4), middle is 5//2 = 2.
        # For a list of 6 nodes (0,1,2,3,4,5), middle is 6//2 = 3 (the second middle node).
        middle_index = count // 2
        
        # Reset the pointer to the beginning of the list for the second pass.
        current = head
        # Traverse half the list to reach the designated middle node.
        for _ in range(middle_index):
            current = current.next
            
        return current

```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    We traverse the list twice. The first pass takes N steps to count the nodes. The second pass takes N/2 steps to reach the middle. The total time is O(N + N/2), which simplifies to O(N).

*   **Space Complexity: O(1)**
    We only use a few variables (`count`, `current`, `middle_index`) for storage, which does not depend on the size of the linked list.

#### **3. Optimized Approach: Two Pointers - Fixed Separation (Nth Node from End)**
**Intuition:**
The two-pass brute force approach is inefficient. We can find the middle node in a single pass using the **Fast and Slow Pointer** technique, a classic variation of the Two Pointer pattern.

The idea is to have two pointers, `slow` and `fast`, both starting at the `head`. In each iteration, the `slow` pointer advances by one step, while the `fast` pointer advances by two steps. Because the `fast` pointer moves at double the speed, by the time it reaches the end of the list, the `slow` pointer will have traveled exactly half the distance. This positions the `slow` pointer perfectly at the middle node.

Let's walk through an example: `1 -> 2 -> 3 -> 4 -> 5`
- **Initial:** `slow` is at 1, `fast` is at 1.
- **Step 1:** `slow` moves to 2. `fast` moves to 3.
- **Step 2:** `slow` moves to 3. `fast` moves to 5.
- **End:** `fast.next` is now `null`, so the loop terminates. The `slow` pointer is at node 3, which is the middle.

This works for even-length lists as well, correctly identifying the second middle node as required.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def middleNode(self, head: Optional[ListNode]) -> Optional[ListNode]:
        # Initialize two pointers, both starting at the head of the list.
        slow = head
        fast = head
        
        # The core of the fast/slow pointer pattern.
        # We must check for 'fast' and 'fast.next' to safely advance the fast pointer by two.
        # This condition naturally handles both odd and even length lists.
        while fast and fast.next:
            # Slow pointer moves one step at a time.
            slow = slow.next
            # Fast pointer moves two steps at a time.
            fast = fast.next.next
            
        # When the loop ends, the fast pointer has reached the end,
        # and the slow pointer is now at the middle.
        return slow
```

**Complexity Analysis:**

*   **Time Complexity: O(N)**
    Although we use two pointers, we only traverse the list once. The `fast` pointer reaches the end in N/2 steps, making the overall time complexity linear to the number of nodes. This is significantly faster in practice than the O(1.5N) of the brute-force method.

*   **Space Complexity: O(1)**
    We only need memory for the two pointers (`slow` and `fast`), resulting in constant space usage.

#### **4. Pattern Connection**
This problem is the quintessential example of the **Fast and Slow Pointer** technique, which is a powerful variant of the broader Two Pointers pattern.

The key signal for this pattern is when you need to find a node based on its **relative position** within a sequence (like the middle, the Nth from the end, or the start of a cycle) without first knowing the sequence's total length. A single pointer can only tell you about its immediate location. However, by introducing a second pointer and controlling the *relative speed* between the two, you can deduce positional information about the entire structure in a single pass. In this case, by setting one pointer's speed to be double the other's, we guarantee that when the `fast` pointer finishes, the `slow` pointer is at the halfway mark. This elegant single-pass solution is far more efficient than the naive two-pass approach and perfectly showcases the power of using two pointers in tandem.

---

---
<a id="boats-to-save-people"></a>
### **881. Boats to Save People**
**Link to Problem:** [https://leetcode.com/problems/boats-to-save-people/](https://leetcode.com/problems/boats-to-save-people/)

#### **1. Problem Statement**
You are given an array `people` where `people[i]` is the weight of the i-th person, and a `limit` which is the maximum weight a boat can carry. Each boat can carry at most two people, provided their combined weight is not over the limit. The goal is to return the minimum number of boats required to save everyone.

#### **2. Brute Force Approach**
**Intuition:**
A straightforward but inefficient approach is to try and pair people up. We can sort the people by weight first, which usually simplifies problems involving combinations. A greedy idea is to take the heaviest person remaining and see if we can pair them with the heaviest possible partner who fits in the boat.

The logic would be:
1. Sort the `people` array in ascending order.
2. Create a boolean array `used` to keep track of who has been placed in a boat.
3. Iterate from the heaviest person down to the lightest (`i` from `n-1` to `0`).
4. If the current person `i` hasn't been used yet, they must take a new boat. Increment the boat count. Mark them as used.
5. Now, try to find a partner for them. Iterate from `i-1` down to `0` (`j`). Find the first (and therefore heaviest) unused person `j` such that `people[i] + people[j] <= limit`.
6. If a partner is found, mark them as used and break the inner loop to continue with the next heaviest person.

This approach is slow because for each person, we might have to scan a large portion of the array again to find a suitable partner.

**Python Code:**
```python
def numRescueBoats_brute_force(people: list[int], limit: int) -> int:
    # 1. Sorting helps in applying a greedy strategy.
    people.sort()
    n = len(people)
    
    # 2. 'used' array to track who has been assigned a boat.
    used = [False] * n
    boats = 0
    
    # 3. Iterate from the heaviest person to the lightest.
    for i in range(n - 1, -1, -1):
        # If this person is already in a boat with someone heavier, skip.
        if used[i]:
            continue
        
        # This person needs a boat.
        boats += 1
        used[i] = True
        
        # 4. Try to find the heaviest possible partner for this person.
        partner_found = False
        for j in range(i - 1, -1, -1):
            if not used[j] and people[i] + people[j] <= limit:
                # Found a partner, put them in the same boat.
                used[j] = True
                partner_found = True
                break # A boat can hold at most two people.
                
    return boats

```
**Complexity Analysis:**

*   **Time Complexity:** `O(N^2)`. The initial sort takes `O(N log N)`. However, the nested loops dominate. The outer loop runs `N` times, and the inner loop can also run up to `N` times in the worst case, leading to an `O(N^2)` complexity.
*   **Space Complexity:** `O(N)`. We use an additional boolean array `used` of size `N` to store the status of each person.

---

### **3. Optimized Approach: Pattern 1: Two Pointers - Converging (Sorted Array Target Sum)**
**Intuition:**
The brute-force approach is slow because of the nested search for a partner. We can optimize this by realizing a crucial greedy insight:

**The heaviest person is the most difficult to pair up.** They have the least amount of "weight capacity" remaining in their boat. If anyone can be paired with the heaviest person, it must be the lightest person.

This insight leads directly to the **Two Pointers - Converging** pattern:
1.  **Sort the array.** This allows us to easily access the lightest and heaviest people.
2.  Initialize two pointers: `left` at the start of the array (lightest person) and `right` at the end (heaviest person).
3.  In each step, we decide the fate of the heaviest person (`people[right]`). They *must* get on a boat. The only question is whether they go alone or with a partner.
4.  We check if the lightest person (`people[left]`) can fit in the boat with the heaviest person (`people[right]`).
    *   If `people[left] + people[right] <= limit`, they can share a boat. We count this boat and move both pointers inward (`left++`, `right--`).
    *   If `people[left] + people[right] > limit`, the lightest person is still too heavy to be paired with the heaviest. This implies *no one* can be paired with the heaviest person (since everyone else is heavier than `people[left]`). Therefore, the heaviest person must take a boat alone. We count this boat and move only the `right` pointer inward (`right--`).
5.  We repeat this process until the pointers cross (`left > right`), at which point everyone has been assigned a boat.

**Example Walkthrough:** `people = [3, 5, 3, 4]`, `limit = 5`
1.  **Sort:** `people` becomes `[3, 3, 4, 5]`.
2.  **Initialize:** `left = 0`, `right = 3`, `boats = 0`.
3.  **Loop 1:**
    *   Pointers: `left` at `3`, `right` at `5`.
    *   Check: `3 + 5 = 8`, which is `> limit`.
    *   Decision: The heaviest person (5) must go alone.
    *   Update: `boats = 1`, `right` moves to `2`. Pointers are now at `left=0`, `right=2`.
4.  **Loop 2:**
    *   Pointers: `left` at `3`, `right` at `4`.
    *   Check: `3 + 4 = 7`, which is `> limit`.
    *   Decision: The heaviest remaining person (4) must go alone.
    *   Update: `boats = 2`, `right` moves to `1`. Pointers are now at `left=0`, `right=1`.
5.  **Loop 3:**
    *   Pointers: `left` at `3`, `right` at `3`.
    *   Check: `3 + 3 = 6`, which is `> limit`.
    *   Decision: The heaviest remaining person (the second 3) must go alone.
    *   Update: `boats = 3`, `right` moves to `0`. Pointers are now at `left=0`, `right=0`.
6.  **Loop 4:**
    *   Pointers: `left = 0`, `right = 0`. They point to the same person (the first 3).
    *   The loop condition `left <= right` is true.
    *   This last person needs a boat.
    *   Update: `boats = 4`, `right` moves to `-1`.
7.  The loop terminates as `left > right`. Final answer: 4 boats.

**Python Code:**
```python
def numRescueBoats(people: list[int], limit: int) -> int:
    # The initial sort is the key prerequisite for the two-pointer approach.
    people.sort()
    
    # Initialize pointers at the two ends of the sorted array.
    left = 0
    right = len(people) - 1
    boats = 0
    
    # Pointers will converge towards the middle.
    while left <= right:
        # Each iteration accounts for one boat, carrying the heaviest person.
        boats += 1
        
        # Check if the lightest person can share the boat with the heaviest.
        if people[left] + people[right] <= limit:
            # If they fit, the lightest person is also on this boat.
            # Move the left pointer inward as this person is now saved.
            left += 1
        
        # The heaviest person is always on the boat in this iteration,
        # so we always move the right pointer inward.
        right -= 1
        
    return boats
```

**Complexity Analysis:**

*   **Time Complexity:** `O(N log N)`. The dominant operation is the initial sorting of the `people` array. The `while` loop runs in `O(N)` time because the `left` and `right` pointers will collectively scan the entire array exactly once.
*   **Space Complexity:** `O(1)` or `O(log N)` to `O(N)`. The space used by the algorithm itself is constant (`left`, `right`, `boats`). However, the space complexity of the sort function in most standard libraries (like Python's Timsort) can range from `O(log N)` to `O(N)`, which should be noted.

---

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging** pattern. The signals that point to this pattern are:

1.  **A Sorted Array is Beneficial:** The problem becomes much easier to reason about when the input is sorted. This allows for greedy decisions, as we can instantly access the lightest and heaviest remaining elements.
2.  **Decisions from the Extremes:** The optimal solution involves making decisions by pairing or comparing elements from the opposite ends of the sorted array (the lightest and the heaviest).
3.  **Converging Pointers:** The problem is solved by maintaining a `left` and `right` pointer that start at the ends and move towards each other, shrinking the problem space with each step until they meet or cross.
4.  **Target Condition:** The core logic `people[left] + people[right] <= limit` is a variation of the "Target Sum" sub-pattern. Instead of looking for an exact sum, we are checking if the sum falls within a certain condition (less than or equal to `limit`), which is a common task for this pattern.

By recognizing that the optimal pairing strategy involves the two extremes (heaviest and lightest), you can quickly identify that a two-pointer approach will be far more efficient than any brute-force method involving nested loops.

---

---
<a id="squares-of-a-sorted-array"></a>
### **977. Squares of a Sorted Array**
**Link to Problem:** [https://leetcode.com/problems/squares-of-a-sorted-array/](https://leetcode.com/problems/squares-of-a-sorted-array/)

#### **1. Problem Statement**
Given an integer array `nums` that is sorted in non-decreasing order, the task is to return a new array containing the squares of each number, also sorted in non-decreasing order.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to follow the problem statement literally. First, we create a new list containing the square of each number from the input array. This new list, however, will not necessarily be sorted. For example, if the input is `[-4, -1, 0, 3]`, the squared list becomes `[16, 1, 0, 9]`. The second step, therefore, is to sort this list of squares to produce the final result.

**Python Code:**
```python
import collections

def sortedSquares_brute_force(nums: list[int]) -> list[int]:
    """
    Solves the problem by first squaring every element and then sorting the result.
    """
    # Step 1: Create a new array by squaring each element from the input array.
    # A list comprehension is a concise way to do this.
    squares = [num * num for num in nums]
    
    # Step 2: Sort the newly created list of squares.
    # Python's built-in sort() method is highly optimized (Timsort).
    squares.sort()
    
    return squares

# Example usage:
# nums = [-4, -1, 0, 3, 10]
# print(sortedSquares_brute_force(nums))  # Output: [0, 1, 9, 16, 100]
```
**Complexity Analysis:**

*   **Time Complexity: O(N log N)**
    The process of squaring each of the N elements takes O(N) time. However, the subsequent sorting step dominates the runtime. A standard comparison-based sorting algorithm, like Timsort (used in Python), has an average and worst-case time complexity of O(N log N).

*   **Space Complexity: O(N)**
    We create a new array, `squares`, to store the squared values, which requires space proportional to the number of elements in the input array. Note: In Python, `sorted()` would create a new list (O(N)), while `.sort()` sorts in-place (O(log N) or O(N) space depending on implementation). Since we already created a new list for the squares, the space complexity is O(N).

---

#### **3. Optimized Approach: [Pattern 1: Two Pointers - Converging]**
**Intuition:**
The brute-force approach has an O(N log N) complexity because it ignores a crucial piece of information: the input array is already sorted. We can leverage this property to build the sorted output array in a single pass, achieving a more optimal O(N) time complexity.

The key observation is that after squaring, the largest values will come from the numbers with the largest *absolute* values. Since the original array `nums` is sorted, these numbers with the largest magnitudes must be at the ends of the array.

For example, in `[-4, -1, 0, 3, 10]`, the candidates for the largest squared value are `(-4)^2 = 16` and `10^2 = 100`. The larger one is `100`. This will be the largest element in our final sorted array. The next largest will be a comparison between `(-4)^2 = 16` and `3^2 = 9`, and so on.

This insight leads directly to the **Two Pointers** pattern. We can place one pointer (`left`) at the beginning of the array and another (`right`) at the end. We compare the squares of the numbers at these two pointers and place the larger square at the end of our result array. We then move the pointer corresponding to the larger square inward and repeat the process, filling the result array from right to left (largest to smallest).

**Walkthrough with `nums = [-4, -1, 0, 3, 10]`:**
1.  Initialize `left = 0`, `right = 4`, and an empty result array `result = [0, 0, 0, 0, 0]`. We'll fill `result` from its last index, `k = 4`.
2.  `left_sq = (-4)^2 = 16`, `right_sq = 10^2 = 100`.
    *   `right_sq` is larger. Place `100` at `result[4]`.
    *   `result` is now `[0, 0, 0, 0, 100]`.
    *   Decrement `right` to 3 and `k` to 3.
3.  `left_sq = (-4)^2 = 16`, `right_sq = 3^2 = 9`.
    *   `left_sq` is larger. Place `16` at `result[3]`.
    *   `result` is now `[0, 0, 0, 16, 100]`.
    *   Increment `left` to 1 and decrement `k` to 2.
4.  `left_sq = (-1)^2 = 1`, `right_sq = 3^2 = 9`.
    *   `right_sq` is larger. Place `9` at `result[2]`.
    *   `result` is now `[0, 0, 9, 16, 100]`.
    *   Decrement `right` to 2 and `k` to 1.
5.  `left_sq = (-1)^2 = 1`, `right_sq = 0^2 = 0`.
    *   `left_sq` is larger. Place `1` at `result[1]`.
    *   `result` is now `[0, 1, 9, 16, 100]`.
    *   Increment `left` to 2 and decrement `k` to 0.
6.  Now `left` (2) and `right` (2) point to the same element.
    *   The loop continues. Place `0^2 = 0` at `result[0]`.
    *   The loop terminates as `left` becomes greater than `right`. The final `result` is `[0, 1, 9, 16, 100]`.

**Python Code:**
```python
def sortedSquares_two_pointers(nums: list[int]) -> list[int]:
    """
    Solves the problem in O(N) time using the two-pointer technique.
    """
    n = len(nums)
    # Initialize a result array of the same size, filled with zeros.
    result = [0] * n
    
    # Initialize two pointers, one at the start and one at the end of the input array.
    left, right = 0, n - 1
    
    # We will fill the result array from the end (largest to smallest).
    # 'k' is the pointer for the last available position in the result array.
    k = n - 1
    
    # The pointers will converge towards the center.
    while left <= right:
        left_square = nums[left] * nums[left]
        right_square = nums[right] * nums[right]
        
        # Compare the squares of the values at the two pointers.
        if left_square > right_square:
            # The left value's square is larger, so it belongs at the end of the sorted output.
            result[k] = left_square
            # Move the left pointer inward.
            left += 1
        else:
            # The right value's square is larger or equal, so it belongs at the end.
            result[k] = right_square
            # Move the right pointer inward.
            right -= 1
        
        # Move the result array's fill position to the left.
        k -= 1
            
    return result

# Example usage:
# nums = [-4, -1, 0, 3, 10]
# print(sortedSquares_two_pointers(nums)) # Output: [0, 1, 9, 16, 100]
```
**Complexity Analysis:**

*   **Time Complexity: O(N)**
    We use two pointers, `left` and `right`, that iterate through the array. The `left` pointer only moves forward, and the `right` pointer only moves backward. In total, they will make N steps. Since we iterate through the array only once, the time complexity is linear.

*   **Space Complexity: O(N)**
    We are creating a `result` array of the same size as the input array. Therefore, the space required is proportional to the input size. (This is often stated as O(1) *auxiliary* space if the space for the output array is not counted, but it's clearer to state O(N) as per standard problem constraints).

---

#### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers - Converging** pattern, adapted for building a new sorted array rather than finding a pair with a specific property. The signals that point to this pattern are:

1.  **Sorted Input Array:** The most crucial prerequisite. The fact that `nums` is sorted guarantees that the elements with the largest absolute values are at the extremes. This property is what allows us to make a definite decision about the largest remaining element by only looking at the two ends.

2.  **Creating a Sorted Output from a Sorted Input:** The task is not to search, but to transform and re-sort. Whenever you need to create a new sorted array from an already sorted one, a two-pointer approach should be a primary consideration as it can often avoid a full O(N log N) sort.

While often associated with finding a target sum, the core principle of the "Converging Pointers" pattern is broader: **efficiently processing a sorted array from its opposite ends.** In this case, we use it to compare the "end" candidates (`nums[left]` and `nums[right]`) to determine which one produces the next value for our sorted result, effectively merging the negative and positive portions of the array into a final sorted list in a single, linear pass.

---

---
<a id="delete-the-middle-node-of-a-linked-list"></a>
### **2095. Delete the Middle Node of a Linked List**
**Link to Problem:** [https://leetcode.com/problems/delete-the-middle-node-of-a-linked-list/](https://leetcode.com/problems/delete-the-middle-node-of-a-linked-list/)

#### **1. Problem Statement**
You are given the `head` of a singly linked list. The task is to find the middle node of this list, delete it, and return the `head` of the modified list. The middle node is defined as the `floor(n / 2)`-th node from the beginning (0-indexed) where `n` is the total number of nodes.

#### **2. Brute Force Approach**
**Intuition:**
The most straightforward way to solve this is to follow the definition directly. To find the middle node at index `floor(n/2)`, we first need to know the total number of nodes, `n`. This suggests a two-pass approach:

1.  **First Pass:** Traverse the entire linked list from beginning to end, simply to count the total number of nodes, `n`.
2.  **Second Pass:** Calculate the index of the node *before* the middle one (`middle_index - 1`). Traverse the list again from the `head`, stopping at this predecessor node.
3.  **Deletion:** Once at the predecessor, update its `next` pointer to skip over the middle node, effectively deleting it from the list.

A special case is a list with only one node, where deleting the middle node results in an empty list.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def deleteMiddle(self, head: Optional[ListNode]) -> Optional[ListNode]:
        # Edge Case: If the list is empty or has only one node,
        # deleting the middle node results in an empty list.
        if not head or not head.next:
            return None

        # --- First Pass: Count the total number of nodes ---
        count = 0
        current = head
        while current:
            count += 1
            current = current.next

        # --- Second Pass: Find the node *before* the middle ---
        # The middle node is at index floor(count / 2).
        # We need to stop at the node at index (middle - 1) to delete the middle.
        middle_predecessor_index = (count // 2) - 1
        
        # Reset pointer to the head for the second traversal.
        current = head
        for _ in range(middle_predecessor_index):
            current = current.next
            
        # --- Deletion ---
        # `current` is now the node just before the middle node.
        # We bypass the middle node by pointing `current.next` to the node after the middle.
        current.next = current.next.next

        return head

```
**Complexity Analysis:**

*   **Time Complexity:** O(N). We traverse the list once to count the nodes (N steps) and then traverse it up to halfway again to find the predecessor (N/2 steps). O(N + N/2) simplifies to O(N).
*   **Space Complexity:** O(1). We only use a few extra variables (`count`, `current`, etc.), so the space usage is constant.

### **3. Optimized Approach: Two Pointers - Fixed Separation (Nth Node from End)**
**Intuition:**
The brute-force approach requires two full or partial passes. We can optimize this to a single pass using the **Slow and Fast Pointer** technique, which is a classic application of this pattern.

The idea is to have two pointers, `slow` and `fast`, both starting at the `head`. The `fast` pointer moves two steps at a time, while the `slow` pointer moves one step at a time. By the time the `fast` pointer reaches the end of the list, the `slow` pointer will be positioned exactly at the middle node.

Why does this work? The `fast` pointer covers twice the distance of the `slow` pointer in the same amount of time. When `fast` has traversed the entire list (length `n`), `slow` will have traversed half the list (length `n/2`), landing it right on the middle node.

To *delete* the middle node, we need access to the node *before* it. We can achieve this by keeping a third pointer, `prev`, that always trails one step behind `slow`. When the loop terminates, `slow` is on the middle node, and `prev` is on the node right before it, ready for the deletion.

Let's walk through `[1, 3, 4, 7, 1, 2, 6]`:
- **Initial:** `prev = None`, `slow = 1`, `fast = 1`
- **Step 1:** `fast` moves to `4`, `slow` moves to `3`, `prev` moves to `1`.
- **Step 2:** `fast` moves to `1`, `slow` moves to `4`, `prev` moves to `3`.
- **Step 3:** `fast` moves to `6`, `slow` moves to `7`, `prev` moves to `4`.
- **End:** `fast.next` is now `None`. The loop stops. `slow` is at `7` (the middle), and `prev` is at `4` (the predecessor). We can now execute `prev.next = slow.next`.

**Python Code:**
```python
# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, val=0, next=None):
#         self.val = val
#         self.next = next

class Solution:
    def deleteMiddle(self, head: Optional[ListNode]) -> Optional[ListNode]:
        # Edge Case: An empty list or a list with one node becomes an empty list.
        if not head or not head.next:
            return None

        # Initialize two pointers, slow and fast.
        # Fast moves twice as fast as slow.
        slow = head
        fast = head
        
        # prev will point to the node right before the slow pointer.
        # This is the node we need to modify for deletion.
        prev = None

        # --- Single Pass Traversal ---
        while fast and fast.next:
            # The fast pointer moves two steps.
            fast = fast.next.next
            
            # Update prev to slow's current position *before* moving slow.
            prev = slow
            
            # The slow pointer moves one step.
            slow = slow.next
            
        # When the loop ends, `slow` is at the middle node.
        # `prev` is at the node just before the middle.
        
        # Delete the middle node by linking the previous node to the next one.
        prev.next = slow.next
        
        return head
```
**Complexity Analysis:**

*   **Time Complexity:** O(N). Although we have two pointers, they both traverse the list in a single pass. The `fast` pointer determines the runtime, which is proportional to the list length, N.
*   **Space Complexity:** O(1). We only use three pointer variables, regardless of the size of the list.

### **4. Pattern Connection**
This problem is a quintessential example of the **Two Pointers with Fixed (or Relative) Separation** pattern, specifically the "Slow and Fast Pointer" variation used for linked lists.

The signal for this pattern is any problem that requires finding a node at a relative position (like the middle, 1/3, etc.) or a fixed distance from the end of a list, without first knowing its size. The brute-force method's need for a preliminary "counting" pass is a strong indicator that a single-pass two-pointer solution exists.

By setting up a fixed relationship between the pointers' speeds (`fast` moves at 2x the speed of `slow`), we can find the halfway point in a single traversal. The core idea is that one pointer's journey across the entire structure gives us the exact location of another pointer at a fractional position. This elegant, single-pass solution is far more efficient than the two-pass brute-force approach, making it a crucial technique for linked list problems.

---

